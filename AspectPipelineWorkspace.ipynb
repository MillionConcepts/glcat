{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reorganizing (and multithreading) aspect refinement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0dd42",
   "metadata": {},
   "source": [
    "handler function for single eclipse \n",
    "\n",
    "function for single frame of eclipse \n",
    "\n",
    "function for \"normal\" frame or \"slew\" frame \n",
    "\n",
    "function to collate aspect solutions from all frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d63fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/bekah/gPhoton2')\n",
    "from gPhoton.reference import get_legs, titular_legs, check_eclipse, eclipse_to_paths\n",
    "from gPhoton.types import Pathlike, GalexBand\n",
    "from typing import Any, Callable, Sequence, Mapping, Optional, Literal\n",
    "from pyarrow import parquet \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/bekah/gPhoton2'\n",
    "eclipse = 815 \n",
    "band = \"NUV\"\n",
    "\n",
    "tab= refine_eclipse(eclipse, band, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tab['frame_type'],tab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0465b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf839d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main handlers \n",
    "\n",
    "def refine_eclipse(\n",
    "        eclipse: int, \n",
    "        band: str, \n",
    "        aspect_root: str, \n",
    "        ext=\"gzip\"): \n",
    "    \"\"\" main pipeline for processing an eclipse \"\"\"\n",
    "    \n",
    "    # setup to run pipeline by getting relevant info & paths \n",
    "    metadata_paths = metadata_filepaths(root)\n",
    "    eclipse_info = get_eclipse_info(eclipse, metadata_paths)\n",
    "    \n",
    "    # paths for photonlist, possible output images etc \n",
    "    # function is from gPhoton2 \n",
    "    paths = get_base_file_paths(\n",
    "                eclipse=eclipse,\n",
    "                band=band,\n",
    "                ext=ext,\n",
    "                root=aspect_root+'/test_data',\n",
    "                mode='direct',\n",
    "                legs=eclipse_info['actual_legs'])\n",
    "    \n",
    "    # get list of frames for all legs into a single dataframe, \n",
    "    # with paths for each backplane \n",
    "    frame_list = get_frame_list(eclipse, metadata_paths)\n",
    "    modified_frame_list = pd.DataFrame()\n",
    "    if eclipse_info['actual_legs'] == 0: \n",
    "            modified_frame_list = get_backplane_filenames(\n",
    "                                        eclipse_info,\n",
    "                                        frame_list,\n",
    "                                        paths[0],\n",
    "                                        0)        \n",
    "    for leg in range(eclipse_info['actual_legs']): \n",
    "            files = get_backplane_filenames(\n",
    "                        eclipse_info,\n",
    "                        frame_list,\n",
    "                        paths[leg],\n",
    "                        leg)\n",
    "            modified_frame_list = pd.concat([modified_frame_list, files], axis=0)\n",
    "    \n",
    "    #for frame in range(len(modified_frame_list)): \n",
    "            #aspect, warnings = refine_frame(modified_frame_list.iloc[frame])\n",
    "            # aspect is tuple of ra, dec, roll, time \n",
    "            3#TODO: do I want the new aspect to go in the old mod_frame_list or it's own file?\n",
    "            #modified_frame_list.iloc\n",
    "            \n",
    "    return modified_frame_list\n",
    "\n",
    "\n",
    "def metadata_filepaths(root: str): \n",
    "    \"\"\"return paths to aspect, boresight, and metadata parquet files \"\"\"\n",
    "    metadata_paths = {}\n",
    "    metadata_paths['og_aspect'] = root + '/gPhoton/aspect/aspect.parquet'\n",
    "    metadata_paths['expanded_aspect'] = root + '/gPhoton/aspect/aspect2.parquet'\n",
    "    # astrom aspect doesn't exist yet but might use it \n",
    "    metadata_paths['astrometry_aspect'] = root + '/gPhoton/aspect/astrom_aspect.parquet'\n",
    "    metadata_paths['boresight'] = root + '/gPhoton/aspect/boresight.parquet'\n",
    "    metadata_paths['metadata'] = root + '/gPhoton/aspect/metadata.parquet'\n",
    "    return metadata_paths \n",
    "    \n",
    "\n",
    "def get_eclipse_info(\n",
    "        eclipse: int, \n",
    "        metadata_paths: dict): \n",
    "    \"\"\"returns a dictionary of info about eclipse\"\"\"\n",
    "    actual, nominal = titular_legs(eclipse)\n",
    "    eclipse_info = {}\n",
    "    eclipse_info['actual_legs'] = actual \n",
    "    eclipse_info['nominal_legs'] = nominal \n",
    "    metadata = parquet.read_table(metadata_paths['metadata'],\n",
    "                              filters=[('eclipse','==',eclipse)]).to_pandas()\n",
    "    eclipse_info['ra_max'] = metadata['ra_max']\n",
    "    eclipse_info['ra_min'] = metadata['ra_min']\n",
    "    eclipse_info['dec_max'] = metadata['dec_max']\n",
    "    eclipse_info['dec_min'] = metadata['dec_min']\n",
    "    eclipse_info['obstype'] = metadata['obstype']\n",
    "    \n",
    "    return eclipse_info \n",
    "    \n",
    "    \n",
    "#TODO: need to check if legs naming starts with 0 or 1  \n",
    "def get_base_file_paths(\n",
    "        eclipse: int,\n",
    "        band: GalexBand = \"NUV\",\n",
    "        depth: Optional[int] = None,\n",
    "        compression: Literal[\"none\", \"gzip\", \"rice\"] = \"gzip\",\n",
    "        root: Pathlike = \"data\",\n",
    "        start: Optional[float] = None,\n",
    "        mode: str = \"direct\",\n",
    "        legs: int = 0,\n",
    "        aperture: Optional[float] = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, str]:\n",
    "    \"\"\"dictionary of dictionaries of file paths for each leg of eclipse \"\"\"\n",
    "    \n",
    "    paths = {} \n",
    "    if legs == 0: \n",
    "        paths[0] = eclipse_to_paths(eclipse=eclipse,band=band,depth=1,\n",
    "                 compression=compression,root=root,mode=mode,leg=legs,) \n",
    "    else: \n",
    "        for i in range(legs): \n",
    "            paths[i] = eclipse_to_paths(eclipse=eclipse,band=band,depth=1,\n",
    "                 compression=compression,root=root,mode=mode,leg=i) \n",
    "    return paths \n",
    "\n",
    "\n",
    "def get_frame_list(\n",
    "        eclipse:int, \n",
    "        metadata_paths: dict): \n",
    "    \"\"\" join extended and og aspect parquet per eclipse to get unrefined \n",
    "    'slew' frames \"\"\"\n",
    "    from pyarrow import parquet \n",
    "    og_aspect = parquet.read_table(metadata_paths['og_aspect'],\n",
    "                                  filters=[('eclipse','==',eclipse)]).to_pandas()\n",
    "    og_aspect['original'] = 'ref'\n",
    "\n",
    "    new_aspect = parquet.read_table(metadata_paths['expanded_aspect'],\n",
    "                                  filters=[('eclipse','==',eclipse)]).to_pandas()\n",
    "    new_aspect = new_aspect.rename(columns={\"pktime\": \"time\"})\n",
    "    new_aspect['original'] = 'slew'\n",
    "\n",
    "    new_aspect = new_aspect.merge(og_aspect, on=\"time\",how=\"outer\") \n",
    "\n",
    "    new_aspect['original_y'].fillna('slew',inplace=True)\n",
    "    new_aspect = new_aspect.rename(columns={\"original_y\": \"frame_type\"})\n",
    "\n",
    "    # get rid of weird distant time stamp at end of file sometimes\n",
    "    # (only works if it's one entry)\n",
    "    if new_aspect.iloc[-1].time - new_aspect.iloc[-2].time > 10: \n",
    "        new_aspect.drop(new_aspect.tail(1).index,inplace=True)\n",
    "    print(new_aspect)\n",
    "    return new_aspect\n",
    "\n",
    "\n",
    "def get_backplane_filenames(\n",
    "        eclipse_info: dict,\n",
    "        frame_list, \n",
    "        paths: dict,\n",
    "        leg: int): \n",
    "    \"\"\" produce df of backplane filenames \"\"\"\n",
    "    phot = parquet.read_table(\n",
    "            paths['photonfile'], columns=['t', 'col', 'row', 'detrad']\n",
    "        )\n",
    "    t_f = phot['t'][0].as_py() # first timestamp of leg from photonlist\n",
    "    t_l = phot['t'][-1].as_py() # last timestamp of leg from photonlist\n",
    "    \n",
    "    #TODO: trying to change this \n",
    "    #leg_frames = frame_list.copy()\n",
    "    leg_frames = frame_list.loc[(frame_list['time'] >= t_f) & (frame_list['time'] <= t_l)].copy()\n",
    "    \n",
    "    # backplanes name formatting fNNNNdd_tNNNNdd\n",
    "    # this is probably an overly complex way to do the naming but ... it works \n",
    "    leg_frames['backplane_path'] = paths['movie'].replace('.fits.gz', f'_dose.fits')\\\n",
    "        .replace('movie','tmovie')\n",
    "    # subtract first timestamp of photonlist \n",
    "    leg_frames['time'] = leg_frames['time']-t_f\n",
    "    leg_frames['time_stamp'] = leg_frames['time'].astype(str).str.split('.').str[0]\\\n",
    "        .str.zfill(4).str.cat(leg_frames['time'].astype(str).str.split('.').str[1].str[:4])\n",
    "    leg_frames['backplane_path'] = leg_frames['backplane_path'].str.split('movie').str[0]\\\n",
    "        .str.cat(leg_frames['time_stamp']).str.cat(leg_frames['backplane_path']\\\n",
    "        .str.split('movie').str[1])\n",
    "    leg_frames['leg'] = leg \n",
    "    # xylist path = l[leg]ts[time stamp].xyls\n",
    "    leg_frames['xylist_path'] = \"l\"+leg_frames['leg'].astype(str)+\"ts\"+\\\n",
    "        (leg_frames['time_stamp'].astype(str))+\".xyls\"\n",
    "    return leg_frames \n",
    "\n",
    "\n",
    "def leg_start_cleanup(): \n",
    "    \"\"\" ok so the first time stamp in the photonlist doesn't necessarily perfectly \n",
    "    match a time stamp in the aspect table. why is that? Idk, it's annoying, might \n",
    "    have to do with some kind of rounding happening somewhere in python. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0444047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits \n",
    "img = fits.open(\"/media/bekah/Extreme Pro/e00815/e00815-nd-b00-f0001-t00180-g_dose.fits.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ac14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].header['NAXIS1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = range(120,150)\n",
    "slopes = [(tab.iloc[t+1]['ra_acs']-tab.iloc[t]['ra_acs'])/(tab.iloc[t+1]['dec_acs']-tab.iloc[t]['dec_acs']) for t in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69720157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tab.iloc[120:150]['roll'],slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCST download \n",
    "\"\"\" Download SCST file from MAST \"\"\"\n",
    "sys.path.insert(0, '/home/bekah/gPhoton2')\n",
    "from gPhoton.io.mast import get_raw_paths \n",
    "# scst files \n",
    "paths = get_raw_paths(10982)\n",
    "paths['scst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scst = fits.open('/home/bekah/Downloads/e10982-scst (1).fits.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c41f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "scst[1].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa820437",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab['time_diff'] = tab['time'].diff()\n",
    "tab['time_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(streaks, frame_series, slew_paths, direction):\n",
    "    \"\"\"looks at ASTRIDE results and uses streak length / direction to\n",
    "    stack .1s frames into a 1s image \"\"\"\n",
    "    layers = 10 # how many .1 s frames we add together, don't have to hard code here\n",
    "    hdul = fits.open(frame_series['backplane_path'])\n",
    "    img1 = hdul[0].data\n",
    "    new_shape = ((layers - 1) * streaks['y_offset'] + img1.shape[0],\n",
    "                 (layers - 1) * streaks['x_offset'] + img1.shape[1])\n",
    "\n",
    "    stacked = np.zeros(new_shape)  # , dtype=np.float)\n",
    "    stacked2 = np.zeros(new_shape)  # , dtype=np.float)\n",
    "    # adding image layers together\n",
    "    #TODO: edit for correct backplane names in loop for short ones\n",
    "    # ADD DIRECTION\n",
    "    for layer in range(layers):\n",
    "        print(\"adding image layers together\")\n",
    "        dose_ais = fits.open(slew_paths['short_backplanes'][layer])\n",
    "        img1 = dose_ais[0].data  # *layer # for tagging pixels as being from a layer\n",
    "        layer_op = (layers - 1) - layer\n",
    "        stacked[layer_op * streaks['y_offset']:layer_op * streaks['y_offset'] + img1.shape[0],\n",
    "        layer_op * streaks['x_offset']:layer_op * streaks['x_offset'] + img1.shape[1]] += img1\n",
    "    # saving image to fits\n",
    "    hdu = fits.PrimaryHDU(stacked)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(slew_paths[\"stacked\"], overwrite=True)\n",
    "    \n",
    "     for layer in range(layers):\n",
    "        print(\"adding image layers together\")\n",
    "        dose_ais = fits.open(slew_paths['short_backplanes'][layer])\n",
    "        img1 = dose_ais[0].data  # *layer # for tagging pixels as being from a layer\n",
    "        layer_op = (layers - 1) - layer\n",
    "        stacked[layer_op * streaks['y_offset']:layer_op * streaks['y_offset'] + img1.shape[0],\n",
    "        layer_op * streaks['x_offset']:layer_op * streaks['x_offset'] + img1.shape[1]] += img1\n",
    "    # saving image to fits\n",
    "    hdu = fits.PrimaryHDU(stacked)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(slew_paths[\"stacked\"], overwrite=True)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(streaks, frame_series, slew_paths, direction):\n",
    "    # num frames \n",
    "    num_frames = 10 \n",
    "\n",
    "    # new image size is the offset between frames * the number of frames being added \n",
    "    # plus the og image size \n",
    "    # num # frames is 10 usually (.1s increments for 1s)\n",
    "    x_dim = ((num_frames - 1)* x_offset + first_frame.shape[0]) \n",
    "    y_dim = ((num_frames - 1)* y_offset + first_frame.shape[1]) \n",
    "\n",
    "    # empty image of stacked dimensions \n",
    "    stacked = np.zeros((x_dim, y_dim))  \n",
    "    stacked_opposite = np.zeros((x_dim, y_dim))  \n",
    "\n",
    "    # iterate through adding 10 .1 s frames \n",
    "    for frame in range(num_frames):\n",
    "        # get frame image \n",
    "        frame_hdul = fits.open(slew_paths['short_backplanes'][frame])\n",
    "        frame_image = frame_hdul[0].data\n",
    "        # countdown layers \n",
    "        inverse_layer = (num_frames - 1) - frame\n",
    "        # add frame to section of stacked image that corresponds with offset \n",
    "        stacked[inverse_layer * streaks['y_offset']:inverse_layer * streaks['y_offset'] + \\\n",
    "                frame_image.shape[0],\n",
    "                inverse_layer * streaks['x_offset']:inverse_layer * streaks['x_offset'] + \\\n",
    "                frame_image.shape[1]] += frame_image\n",
    "        # add frame to section of stacked image that corresponds with offset in opposite dir\n",
    "        stacked_opposite[frame * streaks['y_offset']:frame * streaks['y_offset'] + \\\n",
    "                frame_image.shape[0],\n",
    "                frame * streaks['x_offset']:frame * streaks['x_offset'] + \\\n",
    "                frame_image.shape[1]] += frame_image\n",
    "\n",
    "    # saving image to fits\n",
    "    hdu = fits.PrimaryHDU(stacked)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(slew_paths[\"stacked\"], overwrite=True)\n",
    "    hdu = fits.PrimaryHDU(stacked_opposite)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(slew_paths[\"stacked_opposite\"], overwrite=True)  \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 10 \n",
    "for layer in range(layers):\n",
    "    layer_op = (layers - 1) - layer\n",
    "    print(layer_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b957e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_as = pd.read_csv('new_aspect.csv')\n",
    "back_t = pd.read_csv('backplane_times.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca32ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "photonfile = \"/media/bekah/BekahA/glcat/e00815/e00815-nd-b05.parquet\"\n",
    "\n",
    "phot = parquet.read_table(\n",
    "            photonfile, columns=['t', 'col', 'row', 'detrad']\n",
    "        )\n",
    "t_f = phot['t'][0].as_py() # first timestamp of leg from photonlist\n",
    "t_l = phot['t'][-1].as_py() # last timestamp of leg from photonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f = round(t_f,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75470f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f = round(t_f,3)\n",
    "leg_frames = new_as.loc[(new_as['time'] >= t_f) & (new_as['time'] <= t_l)].copy()\n",
    "leg_frames = leg_frames.round({'time': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_frames.iloc[0]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f039c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_frames.iloc[191]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52baae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "back_t = back_t.round({'time': 3})\n",
    "back_t['time'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_t['time'].iloc[1]-leg_frames.iloc[1]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_as['time'].iloc[1305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_t.iloc[192]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87182778",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = leg_frames.merge(back_t, on=\"time\", how=\"outer\")\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaks = pd.read_csv(\"/media/bekah/BekahA/glcat/e00815/astrom/streaks.txt\", delim_whitespace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15220c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(num, places):\n",
    "    mult = 10 ** places\n",
    "    return int(num * mult) / mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7dfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(740388266.99504,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(740388511.99998,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a142fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(740389281.995,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(740389281.9,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68843785",
   "metadata": {},
   "outputs": [],
   "source": [
    "xylist = fits.open('/media/bekah/BekahA/glcat/e01532/astrom/l0ts01130.xyls')\n",
    "xylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "xylist[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98581783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aspect_correction.util import get_aspect_from_wcsinfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7277e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_aspect_from_wcsinfo('/media/bekah/BekahA/glcat/e00815/astrom/l0ts00040.wcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e6ef1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdu = fits.open('/media/bekah/BekahA/glcat/e01532/astrom/l0ts00360.xyls')\n",
    "hdu[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa619ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdu[0].header['COMMENT'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp815 = pd.read_csv(\"/media/bekah/BekahA/glcat/e00815/00815_new_aspect.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d591db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def astrometry_hostess_run(): \n",
    "    from hostess.subutils import Viewer\n",
    "    solve_process = Viewer.from_command(\n",
    "        \"solve-field\",\n",
    "        \"/media/bekah/BekahA/glcat/e00815/astrom/l0ts00040.xyls\",\n",
    "        overwrite=True,\n",
    "        no_plots=True,\n",
    "        dir_=\"/media/bekah/BekahA/glcat/e00815/astrom/\",\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "        scale_units=\"arcsecperpix\",\n",
    "        scale_low=1.0,\n",
    "        scale_high=1.6,\n",
    "        N=\"none\",\n",
    "        U=\"none\",\n",
    "        B=\"none\",\n",
    "        M=\"none\",\n",
    "        R= \"none\",\n",
    "        _3=120,\n",
    "        _4=120,\n",
    "        temp_axy=True,\n",
    "        crpix_x=600,\n",
    "        crpix_y=600)\n",
    "    solve_process.wait()\n",
    "    return solve_process.done\n",
    "fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3905d08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<invoke.runners.Local at 0x7ff95838c5e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_process.runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33074623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hostess.utilities import timeout_factory\n",
    "waiting, _ = timeout_factory(timeout=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8dd0b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5639586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.621674299240112"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce0c88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.627882480621338"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae981c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.03666853904724"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bbe5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astrometry_hostess_run(): \n",
    "    from hostess.subutils import Viewer\n",
    "    solve_process = Viewer.from_command(\n",
    "        \"solve-field\",\n",
    "        \"/media/bekah/BekahA/glcat/e00815/astrom/l0ts00040.xyls\",\n",
    "        overwrite=True,\n",
    "        no_plots=True,\n",
    "        dir_=\"/media/bekah/BekahA/glcat/e00815/astrom/\",\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "        scale_units=\"arcsecperpix\",\n",
    "        scale_low=1.0,\n",
    "        scale_high=1.6,\n",
    "        N=\"none\",\n",
    "        U=\"none\",\n",
    "        B=\"none\",\n",
    "        M=\"none\",\n",
    "        R= \"none\",\n",
    "        _3=120,\n",
    "        _4=120,\n",
    "        temp_axy=True,\n",
    "        crpix_x=600,\n",
    "        crpix_y=600)\n",
    "    solve_process.wait()\n",
    "    return solve_process.done\n",
    "\n",
    "\n",
    "def time_run(): \n",
    "    from hostess.profilers import DEFAULT_PROFILER as PRO \n",
    "    with PRO.context(\"test_1\"): \n",
    "        astrometry_hostess_run()\n",
    "    return PRO.labels['test_1']['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddfad16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.151"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac7cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
