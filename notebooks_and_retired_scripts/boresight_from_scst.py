import argparse
import csv
import itertools
import multiprocessing
import os
import shelve
import shutil
import sys
import time
import tempfile

from collections import namedtuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, fields as dc_fields
from datetime import datetime, timedelta, timezone
from itertools import chain, combinations, pairwise
from math import isnan, inf as Inf, nan as NaN, pi as PI
from pathlib import Path
from typing import Any, ClassVar, Iterable, Self
from types import UnionType, NoneType

import numpy as np
import pyarrow as pa
import shapely

from astropy.coordinates import angular_separation
from astropy.io import fits
from cartopy import crs
from gPhoton.constants import DETSIZE
from gPhoton.io import mast
from joblib import Memory
from matplotlib.axes import Axes
from matplotlib.figure import Figure
from numpy.typing import NDArray
from pandas import DataFrame
from pyarrow import parquet
from scipy.ndimage import label
from scipy.optimize import minimize
from scipy.sparse import diags_array
from scipy.spatial.distance import pdist
from sklearn.cluster import AgglomerativeClustering

# Conversions between radians and degrees
DEG_TO_RAD = PI/180.0
RAD_TO_DEG = 180.0/PI

# Narrower definitions of the detector size, used only for the extra-strict
# "full exposure" regions in the final output
DETSIZE_370 = DETSIZE * 370 / 400
DETSIZE_350 = DETSIZE * 350 / 400

# Celestial sphere and standard equatorial sky coordinate system
CEL_SPHERE = crs.Globe(
    ellipse=None,
    # these axes produce a sphere whose projections have 1 map unit = 1 degree,
    # which is what aperture_disks etc need
    semimajor_axis=180/PI,
    semiminor_axis=180/PI,
)
CRS_EQUATORIAL = crs.Geodetic(globe=CEL_SPHERE)
PC_EQUATORIAL = crs.PlateCarree(globe=CEL_SPHERE)

# Shapely has no concept of arcs; curves must be approximated using a
# sequence of lines.  The polygon generated by Point(x,y).buffer(r) is
# guaranteed to be inscribed within a true circle of radius r.  The
# approximation can be made more precise by increasing this parameter,
# which is the number of segments to use for each quarter-circle.
#
# At QUAD_SEGS = 72, the approximation of a circle with radius
# APERTURE_RADIUS has area 1262 cdelt^2 units smaller than a true
# circle with that radius; this is a relative error of 0.008%,
# which we should be able to live with.
QUAD_SEGS = 72

# The difference between the GPS epoch (1980-01-06T00:00:00Z) and the
# Unix epoch (1970-01-01T00:00:00Z).  SCST and RAW6 files both contain
# numeric timestamps counting from the GPS epoch.
GPS_EPOCH = datetime(1980, 1, 6, 0, 0, 0, 0, timezone.utc).timestamp()

# Minimum angular separation between the last point of one run of
# aspect fixes, and the first point of the next one, for them to be
# potentially considered separate legs.  Any separation smaller than
# this is assumed to be a recording glitch during a stable longer
# exposure.
#
# Separations of 1′30″ and greater are known to have been
# intentionally used in some eclipses. A histogram of the distribution
# of angular separations between runs has lows at about 30″, 40″, and 50″.
MIN_ANGSEP_FOR_SPLIT = 1/60

# Maximum angular separation etc etc before the two runs under
# consideration are *unconditionally* considered separate legs.
#
# This threshold is set at 8′ because, while the larger "petal pattern"
# eclipses (for which there are legitimate arguments for both
# splitting and combining) can have angular separations between petals
# of up to 20′, above 8′ these are rare compared to cases where the
# telescope accidentally slewed in the middle of the exposure.
#
# Also, keeping this as low as possible reduces the number of cases
# that need to go all the way to cluster-based assignment, and allows
# us to use a higher uncircularity limit, which helps with the "slinky
# pattern" eclipses.
MAX_ANGSEP_FOR_COMBINE = 8/60

# If the uncircularity of a fully exposed region is larger than this,
# try to split the exposure into multiple legs.
UNCIRCULARITY_LIMIT = 0.1

# If the ratio of the area of the partially exposed region to the area
# of the fully exposed region is greater than this, the exposure needs
# to be split into multiple legs.
#
# The partially exposed region can get quite large without its
# actually being a problem, notably for petal-pattern eclipses, and
# the uncircularity limit catches most of the problem cases we care
# about; this mainly exists to deal with the petal-pattern eclipses
# where the pattern kept its radial symmetry but became abnormally
# large.
AREA_RATIO_LIMIT = 1

# There are very few legs this short and they usually consist of just
# one or two points that should have been flagged but weren't.
MIN_LEG_EXPOSURE_TIME = 15

# Size of a diagnostic panel
PANEL_WIDTH = 6


START_TIME = None
def progress(msg: str) -> None:
    now = time.monotonic()

    global START_TIME
    if START_TIME is None:
        START_TIME = now

    elapsed = timedelta(seconds=now - START_TIME)
    sys.stderr.write(f"[{elapsed}]  {msg}\n")


@dataclass(frozen=True, slots=True)
class DownloadedEclipse:
    eclipse: int
    scst_file: Path
    has_aspect: bool
    has_nuv_raw6: bool
    has_fuv_raw6: bool


@dataclass(frozen=True, slots=True)
class SCSTMetadata:
    eclipse: int
    visit: int
    plan_type: str
    plan_subtype: str
    plan_id: int
    planned_legs: int

    eclipse_start: float
    eclipse_duration: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_has_raw6: bool

    legs: NDArray[np.int32]


@dataclass(frozen=True, slots=True)
class AspectFixes:
    time: NDArray[np.float64]
    run_id: NDArray[np.int32]
    ra: NDArray[np.float64]
    dec: NDArray[np.float64]
    n_runs: int
    original_flagged: NDArray[np.bool]
    original_time: NDArray[np.float64]


@dataclass(frozen=True, slots=True)
class ExposureMetric:
    full_area: float
    partial_area_ratio: float
    uncircularity: float

    def is_acceptable(self) -> bool:
        return (
            self.full_area > 0
            and self.partial_area_ratio <= AREA_RATIO_LIMIT
            and self.uncircularity <= UNCIRCULARITY_LIMIT
        )

    def __str__(self) -> str:
        return (
            f"[fa: {self.full_area:g}"
            f" pa_ratio: {self.partial_area_ratio:.3f}"
            f" uncirc: {self.uncircularity:.4g}]"
        )


@dataclass(slots=True, frozen=True)
class MetadataRecord:
    eclipse: int
    plan_type: str
    plan_subtype: str
    visit: int
    plan_id: int
    planned_legs: int
    observed_legs: int | None
    has_aspect: bool

    eclipse_start: float
    eclipse_duration: float
    ok_exposure_time: float | None

    ra0:    float | None
    ra_min: float | None
    ra_max: float | None

    dec0:    float | None
    dec_min: float | None
    dec_max: float | None

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_has_raw6: bool


@dataclass(slots=True, frozen=True)
class BoresightRecord:
    eclipse: int
    leg: int
    time: float
    duration: float

    ra0: float
    ra_min: float
    ra_max: float

    dec0: float
    dec_min: float
    dec_max: float

    planned_ra: float
    planned_dec: float

    full_exposure_area: float
    full_exposure_uncircularity: float
    partial_exposure_area_ratio: float


@dataclass(slots=True, frozen=True)
class LegApertureRecord:
    eclipse: int
    leg: int

    partial: bytes
    full_400: bytes
    full_370: bytes
    full_350: bytes


@dataclass(slots=True, frozen=True)
class Eclipse:
    metadata: MetadataRecord
    boresight: list[BoresightRecord]
    apertures: list[LegApertureRecord]
    category: str
    acceptable: bool


class EclipseCrunchError(RuntimeError):
    def __str__(self) -> str:
        return f"eclipse {self.args[0]}: error: {self.args[1]}"

    def debug_report(self) -> None:
        if hasattr(self.__cause__, "tb"):
            skipping = True
            for line in self.__cause__.tb.splitlines(): # type: ignore
                if skipping:
                    if line == '"""':
                        skipping = False
                else:
                    if line == ("The above exception was the direct cause"
                                " of the following exception:"):
                        break
                    sys.stderr.write(line + "\n")
        else:
            import traceback
            traceback.print_exception(self.__cause__)


def scst_timestamp(stamp: str) -> float:
    """Parse a string timestamp from the format found in SCST file
       headers.  Returns a floating-point count of seconds since the GPS
       epoch.  (This epoch makes them directly comparable to the
       *numeric* timestamps found in the SCST file's *data.*)"""
    return datetime.strptime(stamp, "%y%m%dT%H%M%S%z").timestamp() - GPS_EPOCH


def metadata_from_scst(de: DownloadedEclipse) -> SCSTMetadata:
    """Extract all of the per-eclipse metadata that we care about from
       an SCST file."""
    with fits.open(de.scst_file) as hdulist:
        header = hdulist[0].header
        eclipse = header["ECLIPSE"]

        # For whatever reason, the numeric timestamps in an SCST
        # file's actual data are almost always nnnnnn.995 -- that is,
        # 5000 μs *before* a whole number of seconds since the epoch.
        # The eclipse start time, however, is expressed as a string
        # timestamp with no subsecond information.  Fudge the start
        # time 0.01 s (10,000 μs) earlier to ensure that it will
        # compare strictly less than any data timestamp.
        eclipse_start = scst_timestamp(header["TRANGE0"]) - 0.01

        nuv_det_on_time = header.get('NHVNOMN', 0)
        fuv_det_on_time = header.get('NHVNOMF', 0)
        nuv_det_temp    = header.get('NDTDET', NaN)
        nuv_tdc_temp    = header.get('NDTTDC', NaN)
        fuv_det_temp    = header.get('FDTDET', NaN)
        fuv_tdc_temp    = header.get('FDTTDC', NaN)

        ra0  = header["RA_CENT"]
        dec0 = header["DEC_CENT"]
        ra1  = header.get("RA_1")
        dec1 = header.get("DEC_1")
        if ra1 is not None and ra1 != ra0:
            raise AssertionError(
                f"e{eclipse}: RA_CENT = {ra0} != {ra1} = RA_1")
        if dec1 is not None and dec1 != dec0:
            raise AssertionError(
                f"e{eclipse}: DEC_CENT = {dec0} != {dec1} = DEC_1")

        legs = [ (ra0, dec0) ]
        planned_legs = max(1, header["MPSNPOS"])
        if planned_legs >= 2:
            for i in range(2, planned_legs + 1):
                legs.append((header.get(f"RA_{i}"), header.get(f"DEC_{i}")))

        return SCSTMetadata(
            eclipse            = eclipse,
            visit              = header["VISIT"],
            plan_type          = header["MPSTYPE"],
            plan_subtype       = header["MPSPLAN"],
            plan_id            = header["PLANID"],
            planned_legs       = planned_legs,

            # As the stop time is also expressed as a string timestamp,
            # it will compare greater than any data timestamp from this
            # eclipse with no need for any fudging.
            eclipse_start      = eclipse_start,
            eclipse_duration   = (scst_timestamp(header["TRANGE1"])
                                  - eclipse_start),

            nuv_det_on_time    = nuv_det_on_time,
            nuv_det_temp       = nuv_det_temp,
            nuv_tdc_temp       = nuv_tdc_temp,
            nuv_has_raw6       = de.has_nuv_raw6,

            fuv_det_on_time    = fuv_det_on_time,
            fuv_det_temp       = fuv_det_temp,
            fuv_tdc_temp       = fuv_tdc_temp,
            fuv_has_raw6       = de.has_fuv_raw6,
            legs               = np.array(legs)
        )


def angular_separation_deg(
    ra0: float | NDArray[np.float64],
    dec0: float | NDArray[np.float64],
    ra1: float | NDArray[np.float64],
    dec1: float | NDArray[np.float64],
) -> NDArray[np.float64]:
    """As astropy.coordinates.angular_separation, but inputs and outputs
       are in degrees rather than radians.  Should not be used with astropy
       Angle or Quantity objects."""
    return RAD_TO_DEG * angular_separation(
        ra0 * DEG_TO_RAD,
        dec0 * DEG_TO_RAD,
        ra1 * DEG_TO_RAD,
        dec1 * DEG_TO_RAD
    )


def angular_separation_points(
    u: NDArray[np.float64],
    v: NDArray[np.float64],
) -> float:
    """As astropy.coordinates.angular_separation, but inputs are
       2-vectors."""
    return angular_separation(u[0], u[1], v[0], v[1])


# <https://skeptric.com/calculate-centroid-on-sphere/> in one
# sentence: The centroid of a pointset on the surface of a sphere is
# a point which minimizes the mean geodesic distance from itself to
# all the points in the pointset.  For the pointsets we care about,
# all within 5° or so of each other, the centroid is always unique.
# (It isn't always unique for arbitrary pointsets; for example, the
# centroid of [north pole, south pole] is any point on the equator.
# However, uniqueness only fails when the pointset is symmetrically
# distributed over the entire sphere.)
def centroid_objective(
    pt: NDArray[np.float64],
    ra_rad: NDArray[np.float64],
    dec_rad: NDArray[np.float64]
) -> float:
    """Return the mean angular separation between point PT and all the
       pairs of points (ra_rad, dec_rad).  PT should be a 2-tuple or
       2-vector.  All angles should be in radians, not degrees, as
       this is what astropy.coordinates.angular_separation wants."""
    return angular_separation(pt[0], pt[1], ra_rad, dec_rad).mean()


def centroid_on_celsphere(
    ra: NDArray[np.float64],
    dec: NDArray[np.float64]
) -> NDArray[np.float64]:
    """Return a point (c_ra, c_dec) which minimizes the mean angular
       separation from itself to all pairs of points (ra, dec)."""
    ra_rad, dec_rad = np.broadcast_arrays(
        ra * DEG_TO_RAD,
        dec * DEG_TO_RAD,
    )
    p0 = np.array([ra_rad[0], dec_rad[0]])
    # Nelder-Mead simplex algorithm does not require gradients
    res = minimize(centroid_objective, p0, args=(ra_rad, dec_rad),
                   method="nelder-mead")
    if not res.success:
        raise RuntimeError(f"Failed to find centroid: {res.message}")
    return res.x.ravel() * RAD_TO_DEG




def plane_projection(ra0: float, dec0: float) -> crs.Projection:
    """Return a projection of the celestial sphere onto a plane,
       centered at equatorial coordinates (ra0, dec0).  The projection
       will be equal-area, and will approximately preserve circles
       near (ra0, dec0).  Presently, a Lambert azimuthal equal area
       projection is used."""
    return crs.LambertAzimuthalEqualArea(
        central_longitude=ra0,
        central_latitude=dec0,
        globe=CEL_SPHERE,
    )


def ok_aspect_fixes(aspect: DataFrame) -> AspectFixes:
    """extract the time of each aspect fix from the aspect table,
       calculate whether each aspect fix is considered part of the
       observation, and return (time, run_id, ra, dec) for each
       fix that is considered part of the observation."""
    # if the mysterious 'flags' number is odd, it means this aspect
    # record should not be considered part of the observation.
    # the full set of reasons for flagging an aspect record is not
    # known, but the most common reasons are that the detector hadn't
    # yet come up to operating voltage, or that the telescope was
    # carrying out a high-speed slew during that time step.
    ok = aspect["flags"].to_numpy() & 1 == 0
    original_flagged = ~ok

    # we also distrust aspect records immediately before and after a
    # flagged aspect record; the first record in 'aspect' is assumed
    # to be preceded by a flagged record, and the last record is
    # assumed to be followed by a flagged record
    ok &= np.concat([[False], ok[:-1]])
    ok &= np.concat([ok[1:], [False]])

    # we also distrust aspect records whose time step is not 1
    times = aspect["time"].to_numpy()
    ok[1:] &= np.isclose(times[1:] - times[:-1], 1.0)

    # give each run of unflagged fixes a serial number
    runs, n_runs = label(ok)

    return AspectFixes(
        time   = times[ok],
        # label() used 0 for flagged points and [1, n_runs] for
        # unflagged points; we're throwing away all the unflagged
        # points, so shift the labels to [0, n_runs), aligned with
        # what you get from range(n_runs).
        run_id = runs[ok] - 1,
        ra     = aspect["ra"].to_numpy()[ok],
        dec    = aspect["dec"].to_numpy()[ok],
        n_runs = n_runs,
        original_time = times,
        original_flagged = original_flagged
    )


def aperture_disks(
    ras: NDArray[np.float64],
    decs: NDArray[np.float64],
    diameter: float,
) -> NDArray[np.object_]:
    """For each point `(ra, dec) in zip(ras, decs)`, construct a
       circle in sky coordinates, centered at that point, whose
       radius is the radius of the GALEX detector aperture.
    """
    # disk with the radius of the detector aperture, in degrees
    aperture = shapely.Point(0, 0).buffer(diameter/2, quad_segs=QUAD_SEGS)

    # For each point (ra, dec), back-project 'aperture', from an
    # azimuthal equidistant projection centered on (ra, dec), to the
    # celestial sphere.
    pc = PC_EQUATORIAL
    globe = CEL_SPHERE
    aeqd = crs.AzimuthalEquidistant
    return np.array([
        pc.project_geometry(
            aperture,
            aeqd(ra, dec, globe=globe)
        ).buffer(0)
        for ra, dec in zip(ras, decs)
    ])


def exposure_for_apertures(
    disks: NDArray[np.object_]
) -> tuple[
    shapely.Polygon | shapely.MultiPolygon,
    shapely.Polygon | shapely.MultiPolygon,
]:
    """Given a set of disks, each corresponding to the GALEX detector
    aperture at some time step, produce the region, on an imaginary
    detector plate of unbounded extent, that is common to all the
    disks (which is the region that was exposed to incoming light
    during the entire exposure) and the region that is common to some
    or all the disks (exposed to light during at least some of the
    exposure).  Note: this should only be used with 'disks' produced
    by aperture_disks(..., DETSIZE).

    """
    full = shapely.intersection_all(disks)
    partial = shapely.union_all(disks)

    assert isinstance(full, (shapely.Polygon, shapely.MultiPolygon))
    assert isinstance(partial, (shapely.Polygon, shapely.MultiPolygon))

    return full, partial


def exposure_goodness(
    full: shapely.Polygon | shapely.MultiPolygon,
    partial: shapely.Polygon | shapely.MultiPolygon
) -> ExposureMetric:
    """Compute measures of how stable the telescope seems to
    have been during an exposure characterized by the regions
    FULL and PARTIAL.

    The return value has 3 attributes:
     - full_area: Area of the full exposure region in square degrees.
     - partial_area_ratio: Ratio of the area of the partially exposed
       region to the area of the full exposure region.
     - uncircularity: Measure of how far off the full region is from
       a perfect circle.  Specifically, this is the reciprocal
       isoperimetric quotient minus one; zero is a perfect circle.
       See <https://en.wikipedia.org/wiki/Isoperimetric_inequality>.
       Using the reciprocal quotient and subtracting one facilitates
       plotting typical-for-us values on a log scale.
    """

    # for AIS eclipses there might be no overlap at all when
    # considering the entire eclipse at once
    if full.is_empty:
        return ExposureMetric(
            full_area = 0,
            partial_area_ratio = Inf,
            uncircularity = Inf,
        )

    full_area = full.area
    if isinstance(full, shapely.Polygon):
        full_perimeter = full.exterior.length
    else:
        assert isinstance(full, shapely.MultiPolygon)
        full_perimeter = sum(p.exterior.length for p in full.geoms)


    # The standard isoperimetric quotient is 4πA⁄P², which ranges from
    # 0 to 1, where 1 is a perfect circle and 0 is a shape with zero
    # area but nonzero perimeter, e.g. a line segment.  Its reciprocal
    # ranges from 1 to positive infinity, and subtracting 1 puts a
    # perfect circle at 0.  We do this transformation because most
    # legs have an isoperimetric quotient very close to 1; on our data
    # set, the shifted reciprocal quotient ranges from 4 × 10⁻⁵ to
    # 0.21, which is usefully put on a log scale.)

    s_r_isop = full_perimeter * full_perimeter / (4 * PI * full_area) - 1

    # For the "partial area ratio", what we want is the _exclusive_
    # partial area, that is, the area of the partial region minus the
    # area of the full region.
    return ExposureMetric(
        full_area = full_area,
        partial_area_ratio = (partial.area - full_area) / full_area,
        uncircularity = s_r_isop
    )


@dataclass(slots=True) # temporarily not frozen see choose_legs
class LegAssignment:
    category: str
    n_legs: int
    disks: NDArray[np.object_]
    run_masks: list[NDArray[np.bool]]
    grp_masks: list[NDArray[np.bool]]
    leg_masks: list[NDArray[np.bool]]
    leg_metrics: list[ExposureMetric]
    leg_regions: list[tuple[
        shapely.Polygon | shapely.MultiPolygon,
        shapely.Polygon | shapely.MultiPolygon
    ]]

    ALL_CATEGORIES: ClassVar[list[str]] = [
        "forced_full_depth",
        "forced_all_split",
        "all_merged",
        "clustered",
        "no_aspect",
    ]

    CATEGORY_SHORT_NAMES: ClassVar[dict[str, str]] = {
        "forced_full_depth" : "f",
        "forced_all_split"  : "s",
        "all_merged"        : "m",
        "clustered"         : "c",
        "no_aspect"         : "x",
    }

    def is_acceptable(self) -> bool:
        return all(m.is_acceptable() for m in self.leg_metrics)

    def metric(self) -> NDArray[np.float64]:
        return np.array([
            sum(m.uncircularity for m in self.leg_metrics) / self.n_legs,
            sum(m.partial_area_ratio for m in self.leg_metrics) / self.n_legs,
        ])

    @classmethod
    def _from_masks(
        cls,
        disks: NDArray[np.object_],
        leg_masks: list[NDArray[np.bool]],
        grp_masks: list[NDArray[np.bool]],
        run_masks: list[NDArray[np.bool]],
        category: str,
    ) -> Self:
        """Given a set of aperture disks, and a set of mask vectors
        that define legs, construct a complete LegAssignment object.
        CATEGORY must be one of the strings in LegAssignment.ALL_CATEGORIES
        and this is currently not checked.
        """
        regions = []
        metrics = []
        for leg_mask in leg_masks:
            full, partial = exposure_for_apertures(disks[leg_mask])
            metric = exposure_goodness(full, partial)
            regions.append((full, partial))
            metrics.append(metric)
        return cls(
            n_legs = len(leg_masks),
            disks = disks,
            run_masks = run_masks,
            grp_masks = grp_masks,
            leg_masks = leg_masks,
            leg_regions = regions,
            leg_metrics = metrics,
            category = category,
        )

    @classmethod
    def forced_full_depth(
        cls,
        disks: NDArray[np.object_],
        run_masks: list[NDArray[np.bool]],
        grp_masks: list[NDArray[np.bool]],
    ) -> Self:
        return cls._from_masks(
            disks,
            grp_masks,
            grp_masks,
            run_masks,
            "forced_full_depth"
        )

    @classmethod
    def forced_all_split(
        cls,
        disks: NDArray[np.object_],
        run_masks: list[NDArray[np.bool]],
        grp_masks: list[NDArray[np.bool]],
    ) -> Self:
        return cls._from_masks(
            disks,
            grp_masks,
            grp_masks,
            run_masks,
            "forced_all_split"
        )

    @classmethod
    def all_possible_merges(
        cls,
        disks: NDArray[np.object_],
        run_masks: list[NDArray[np.bool]],
        grp_masks: list[NDArray[np.bool]],
        mand_splits: list[bool],
    ) -> Self:
        leg_masks = []
        null_mask = np.full(grp_masks[0].shape, False, np.bool)
        leg_mask = null_mask.copy()
        for grp_mask, split_after in zip(grp_masks, mand_splits):
            leg_mask |= grp_mask
            if split_after:
                leg_masks.append(leg_mask)
                leg_mask = null_mask.copy()

        return cls._from_masks(
            disks,
            leg_masks,
            grp_masks,
            run_masks,
            "all_merged"
        )

    @classmethod
    def clustered(
        cls,
        disks: NDArray[np.object_],
        run_masks: list[NDArray[np.bool]],
        grp_masks: list[NDArray[np.bool]],
        leg_masks: list[NDArray[np.bool]],
    ) -> Self:
        return cls._from_masks(
            disks,
            leg_masks,
            grp_masks,
            run_masks,
            "clustered"
        )


def group_runs_by_separation(
    fixes: AspectFixes,
) -> tuple[list[NDArray[np.bool]], list[NDArray[np.bool]], list[bool]]:
    """Combine consecutive runs when the angular separation between the
       last point of one run and the first point of the next run is less
       than MIN_ANGSEP_FOR_SPLIT.  Conversely, when the gap is greater
       than MAX_ANGSEP_FOR_COMBINE, mark that pair of runs as a mandatory
       split point."""
    run_masks = []
    run_mask = fixes.run_id == 0
    run_masks.append(run_mask)

    grp_masks = []
    grp_mask = run_mask.copy()

    mand_splits = []

    lst_ra = fixes.ra[run_mask][-1]
    lst_dec = fixes.dec[run_mask][-1]
    for run in range(1, fixes.n_runs):
        run_mask = fixes.run_id == run
        run_masks.append(run_mask)
        fst_ra = fixes.ra[run_mask][0]
        fst_dec = fixes.dec[run_mask][0]
        gap = angular_separation_deg(lst_ra, lst_dec, fst_ra, fst_dec)
        if gap >= MIN_ANGSEP_FOR_SPLIT:
            # end the current group after the _previous_ run
            mand_splits.append(gap > MAX_ANGSEP_FOR_COMBINE)
            grp_masks.append(grp_mask)
            grp_mask = run_mask.copy()
        else:
            grp_mask |= run_mask
        lst_ra = fixes.ra[run_mask][-1]
        lst_dec = fixes.dec[run_mask][-1]

    # don't lose the last group
    grp_masks.append(grp_mask)
    # a sentinel makes the logic in choose_legs and all_possible_merges easier
    mand_splits.append(True)

    return run_masks, grp_masks, mand_splits


def clustered_leg_candidates(
    fixes: AspectFixes,
    disks: NDArray[np.object_],
    run_masks: list[NDArray[np.bool]],
    grp_masks: list[NDArray[np.bool]],
    mand_splits: list[bool],
) -> Iterable[LegAssignment]:

    max_legs = len(grp_masks)
    min_legs = sum(mand_splits)

    # If max_legs == min_legs, then all the splits are mandatory and
    # we shouldn't be here.
    assert min_legs < max_legs

    # The assignment that gives every group its own leg can be emitted
    # without doing any actual clustering.
    yield LegAssignment.clustered(disks, run_masks, grp_masks, grp_masks)

    # Compute the centroid of each group.
    grp_centroids = np.array([
        centroid_on_celsphere(fixes.ra[mask], fixes.dec[mask])
        for mask in grp_masks
    ])

    # Compute the connectivity matrix.  Because legs must consist of
    # consecutive runs, this matrix is very sparse: all elements are
    # zero, except on the 1st upper and lower diagonals, which are
    # both equal to the logical negation of the first N-1 elements
    # of 'mand_splits' -- i.e. each group can be merged only with
    # its immediate neighbors, and even then, only if there isn't
    # a mandatory split at that point.
    #
    # (You might be wondering why we bother with clustering when the
    # options for merger are so sparse--but, even so, the number
    # of possibilities grows *factorially* with the number of groups.)
    mergeable = (~np.array(mand_splits[:-1])).astype(np.uint32)
    connectivity = diags_array([mergeable, mergeable], offsets=[1, -1])

    # Save computational effort by memoizing the cluster tree across
    # different cluster sizes; annoyingly, this requires a disk location.
    with tempfile.TemporaryDirectory(prefix="bs-from-scst") as cache_dir:

        # We don't generate the cluster with the minimum possible
        # number of legs, because that's "all possible merges" and
        # our caller already did that.  We also don't generate the
        # cluster with the maximum possible number of legs, because
        # that's "every group in its own leg" which was emitted above.
        # (If there is more than one pair of groups that can be either
        # merged or not merged, then max_legs is at least two more than
        # min_legs.)
        for n_legs in range(min_legs + 1, max_legs):
            labels = AgglomerativeClustering(
                n_clusters = n_legs,
                linkage = "ward",      # best behaved with sparse connectivity
                metric = "euclidean",  # forced by linkage="ward"
                connectivity = connectivity,
                memory = cache_dir,
                compute_full_tree = False,
            ).fit_predict(grp_centroids)

            leg_masks = [np.zeros_like(grp_masks[0]) for _ in range(n_legs)]
            for grp, leg in enumerate(labels):
                leg_masks[leg] |= grp_masks[grp]

            yield LegAssignment.clustered(disks, run_masks,
                                          grp_masks, leg_masks)


def dump_metrics(diag_dir: Path, eclipse: int,
                 metrics: list[tuple[int, float, float, float, bool]]) -> None:
    import csv
    fname = diag_dir / f"{eclipse}-metrics.csv"
    with open(fname, "wt") as fp:
        wr = csv.writer(fp, dialect="unix", quoting=csv.QUOTE_MINIMAL)
        wr.writerow([
            "eclipse", "n.legs", "par", "uncirc", "score", "acceptable"
        ])
        for m in metrics:
            wr.writerow((eclipse, *m))


def choose_legs(
    fixes: AspectFixes,
    eclipse: int,
    diag_dir: Path | None
) -> LegAssignment:
    """Merge runs of aspect fixes from 'fixes' into legs.

    A leg is one or more _consecutive_ runs which produce suitable
    exposure metrics.  The rules are:

    1. If the gap between a pair of consecutive runs is smaller than
       MIN_ANGSEP_FOR_SPLIT, those runs are always combined.

    2. Conversely, if the gap is larger than MAX_ANGSEP_FOR_COMBINE,
       those runs are always split into multiple legs.

    3. If we still have more choices to make after applying rules 1
       and 2, first we try combining as much as we can; if that
       produces a set of legs all of whose metrics are "acceptable"
       (see ExposureMetric.is_acceptable()), then we're done.

    4. Otherwise, we take the centroid of each group of runs that
       we were required to combine by rule 1 as the representative
       of that group, and perform constrained hierarchical clustering
       to produce candidate sets with each possible number of legs.
       The candidate set with the smallest mean uncircularity is chosen,
       whether or not its metrics are all "acceptable".
    """
    run_masks, grp_masks, mand_splits = group_runs_by_separation(fixes)
    disks = aperture_disks(fixes.ra, fixes.dec, DETSIZE)

    assert len(grp_masks) >= 1
    assert len(mand_splits) == len(grp_masks)

    if len(grp_masks) == 1:
        return LegAssignment.forced_full_depth(disks, run_masks, grp_masks)
    if all(mand_splits):
        return LegAssignment.forced_all_split(disks, run_masks, grp_masks)

    cand = LegAssignment.all_possible_merges(disks, run_masks, grp_masks,
                                             mand_splits)
    if cand.is_acceptable():
        return cand

    # Clustering-based candidates have to beat "all possible merges",
    # which we will go ahead and use (even if it isn't "acceptable")
    # if we can't do any better.
    best = cand
    best_metric = best.metric()
    best_score = np.dot(best_metric, best_metric)
    if diag_dir is not None:
        metric_groups = [
            (best.n_legs, *best_metric, best_score, best.is_acceptable())
        ]

    for cand in clustered_leg_candidates(fixes, disks,
                                         run_masks, grp_masks,
                                         mand_splits):
        cand_metric = cand.metric()
        cand_score = np.dot(cand_metric, cand_metric)

        if diag_dir is not None:
            metric_groups.append(
                (cand.n_legs, *cand_metric, cand_score, best.is_acceptable())
            )

        if cand_score < best_score:
            best = cand
            best_score = cand_score

    if diag_dir is not None:
        dump_metrics(diag_dir, eclipse, metric_groups)

    return best


def planned_targets(scst_metadata: SCSTMetadata) -> NDArray[np.float64]:
    planned_ra0 = [c[0] for c in scst_metadata.legs]
    planned_dec0 = [c[1] for c in scst_metadata.legs]
    return np.array([planned_ra0, planned_dec0]).transpose()


def boresight_for_leg(
    eclipse: int,
    leg_index: int,
    planned_tgts: NDArray[np.float64],
    fixes: AspectFixes,
    legs: LegAssignment,
) -> tuple[BoresightRecord, LegApertureRecord]:

    leg_id = leg_index + 1
    leg_mask = legs.leg_masks[leg_index]
    metric = legs.leg_metrics[leg_index]

    ra = fixes.ra[leg_mask]
    dec = fixes.dec[leg_mask]
    ra0, dec0 = centroid_on_celsphere(ra, dec)

    full, partial = legs.leg_regions[leg_index]
    full_370 = shapely.intersection_all(aperture_disks(ra, dec, DETSIZE_370))
    full_350 = shapely.intersection_all(aperture_disks(ra, dec, DETSIZE_350))

    time = fixes.time[leg_mask]

    # 'partial' can be empty if the leg is a single point.
    if not partial.is_empty:
        bbox = partial.bounds
    else:
        assert not full.is_empty
        bbox = full.bounds

    # Find the nearest planned target to (ra0, dec0).  If its
    # angular separation from (ra0, dec0) is less than MAX_ANGSEP_FOR_COMBINE,
    # assign this leg to that target.
    angseps = angular_separation_deg(
        ra0, dec0, planned_tgts[:,0], planned_tgts[:,1]
    )
    minsep_tgt = angseps.argmin()
    if angseps[minsep_tgt] <= MAX_ANGSEP_FOR_COMBINE:
        planned_ra = planned_tgts[minsep_tgt, 0]
        planned_dec = planned_tgts[minsep_tgt, 1]
    else:
        planned_ra = None
        planned_dec = None

    # If possible, expand the time range to ensure that there is one
    # flagged aspect point on either end of the run.  This ensures
    # that gphoton2 doesn't get confused by time ranges that consist
    # _only_ of unflagged points.
    start_ix = 0
    while fixes.original_time[start_ix] < time[0]:
        start_ix += 1
    while start_ix > 0 and not fixes.original_flagged[start_ix]:
        start_ix -= 1

    limit = len(fixes.original_time) - 1
    stop_ix = limit
    while fixes.original_time[stop_ix] > time[-1]:
        stop_ix -= 1
    while stop_ix < limit and not fixes.original_flagged[stop_ix]:
        stop_ix += 1

    # Aspect time points all end in .995 for some reason.  Use this to
    # make the boresight start and end times be strictly less and
    # greater than the time points at either end of the range.
    start_time = fixes.original_time[start_ix] - 0.005   # nnnn.990
    stop_time = fixes.original_time[stop_ix] + 0.005     # nnnm.000
    duration = stop_time + 1 - start_time

    return (
        BoresightRecord(
            eclipse                     = eclipse,
            leg                         = leg_id,
            time                        = start_time,
            duration                    = duration,

            planned_ra                  = planned_ra,
            ra0                         = ra0,
            ra_min                      = bbox[0],
            ra_max                      = bbox[2],

            planned_dec                 = planned_dec,
            dec0                        = dec0,
            dec_min                     = bbox[1],
            dec_max                     = bbox[3],

            full_exposure_area          = metric.full_area,
            full_exposure_uncircularity = metric.uncircularity,
            partial_exposure_area_ratio = metric.partial_area_ratio,
        ),
        LegApertureRecord(
            eclipse  = eclipse,
            leg      = leg_id,
            partial  = partial.wkb,
            full_400 = full.wkb,
            full_370 = full_370.wkb,
            full_350 = full_350.wkb,
        )
    )


def finish_eclipse(
    md: SCSTMetadata,
    planned_tgts: NDArray[np.float64],
    fixes: AspectFixes,
    legs: LegAssignment,
) -> Eclipse:
    boresight_recs = []
    la_recs = []

    for leg in range(legs.n_legs):
        br, la = boresight_for_leg(
            md.eclipse,
            leg,
            planned_tgts,
            fixes,
            legs,
        )
        boresight_recs.append(br)
        la_recs.append(la)


    # compute the centroid and bounding box of the entire eclipse
    ra0, dec0 = centroid_on_celsphere(fixes.ra, fixes.dec)
    full, partial = exposure_for_apertures(legs.disks)

    # partial can be empty if the eclipse is a single point
    if not partial.is_empty:
        bbox = partial.bounds
    else:
        assert not full.is_empty
        bbox = full.bounds

    return Eclipse(
        metadata=MetadataRecord(
            eclipse          = md.eclipse,
            visit            = md.visit,
            plan_type        = md.plan_type,
            plan_subtype     = md.plan_subtype,
            plan_id          = md.plan_id,
            planned_legs     = md.planned_legs,
            observed_legs    = legs.n_legs,
            has_aspect       = True,

            eclipse_start    = md.eclipse_start,
            eclipse_duration = md.eclipse_duration,
            ok_exposure_time = len(fixes.time),

            nuv_det_on_time  = md.nuv_det_on_time,
            nuv_det_temp     = md.nuv_det_temp,
            nuv_tdc_temp     = md.nuv_tdc_temp,
            nuv_has_raw6     = md.nuv_has_raw6,

            fuv_det_on_time  = md.fuv_det_on_time,
            fuv_det_temp     = md.fuv_det_temp,
            fuv_tdc_temp     = md.fuv_tdc_temp,
            fuv_has_raw6     = md.fuv_has_raw6,

            ra0              = ra0,
            ra_min           = bbox[0],
            ra_max           = bbox[2],

            dec0             = dec0,
            dec_min          = bbox[1],
            dec_max          = bbox[3],
        ),
        boresight  = boresight_recs,
        apertures  = la_recs,
        category   = legs.category,
        acceptable = legs.is_acceptable(),
    )


def no_aspect_eclipse(md: SCSTMetadata) -> Eclipse:
    """Return a metadata record, and no boresight or aperture records,
       for an eclipse that is not currently included in the aspect table
       (but does have an SCST file and maybe some raw6 data on MAST)."""
    return Eclipse(
        metadata=MetadataRecord(
            eclipse          = md.eclipse,
            visit            = md.visit,
            plan_type        = md.plan_type,
            plan_subtype     = md.plan_subtype,
            plan_id          = md.plan_id,
            planned_legs     = md.planned_legs,
            observed_legs    = None,
            has_aspect       = True,

            eclipse_start    = md.eclipse_start,
            eclipse_duration = md.eclipse_duration,
            ok_exposure_time = None,

            nuv_det_on_time  = md.nuv_det_on_time,
            nuv_det_temp     = md.nuv_det_temp,
            nuv_tdc_temp     = md.nuv_tdc_temp,
            nuv_has_raw6     = md.nuv_has_raw6,

            fuv_det_on_time  = md.fuv_det_on_time,
            fuv_det_temp     = md.fuv_det_temp,
            fuv_tdc_temp     = md.fuv_tdc_temp,
            fuv_has_raw6     = md.fuv_has_raw6,

            ra0              = None,
            ra_min           = None,
            ra_max           = None,
            dec0             = None,
            dec_min          = None,
            dec_max          = None,
        ),
        boresight  = [],
        apertures  = [],
        category   = "no_aspect",
        acceptable = True,
    )


def autolim_with_geoms(ax: Axes, buffer: float = 0.05) -> None:
    """adjust the limits of axis 'ax' to cover the extent of all the
    projected shapes within that axis"""
    # from https://stackoverflow.com/a/79734123

    from cartopy.mpl.feature_artist import FeatureArtist, _GeomKey
    from matplotlib.collections import PathCollection

    # we need to make sure that the figure has been drawn
    # (so that all reprojected features are properly cached)
    ax.figure.canvas.draw()

    # iterate through all children of the axes and identify cartopy Features
    x0 = Inf
    y0 = Inf
    x1 = -Inf
    y1 = -Inf

    for c in ax._children:
        if isinstance(c, FeatureArtist):
            for g in c._feature.geometries():
                # get feature boundary from cached reprojected geometries
                m = c._geom_key_to_path_cache[_GeomKey(g)]
                p = m[ax.projection]
                xg0, yg0 = p.vertices.min(axis=0)
                xg1, yg1 = p.vertices.max(axis=0)
                x0 = min(x0, xg0)
                x1 = max(x1, xg1)
                y0 = min(y0, yg0)
                y1 = max(y1, yg1)
        elif isinstance(c, PathCollection):
            for p in c.get_paths():
                xg0, yg0 = p.vertices.min(axis=0)
                xg1, yg1 = p.vertices.max(axis=0)
                x0 = min(x0, xg0)
                x1 = max(x1, xg1)
                y0 = min(y0, yg0)
                y1 = max(y1, yg1)
        else:
            # get normal artist bounding box
            xg0, yg0, xg1, yg1 = c.get_bbox().bounds
            x0 = min(x0, xg0)
            x1 = max(x1, xg1)
            y0 = min(y0, yg0)
            y1 = max(y1, yg1)

    # add a bit of buffer around the bounding box
    dx, dy = (x1 - x0) * buffer, (y1 - y0) * buffer
    ax.set_extent([x0 - dx,  x1 + dx,  y0 - dy, y1 + dy], ax.projection)


def plot_targets(ax: Axes, points: Any, color: str, label: str) -> None:
    # can only apply one marker shape per call to scatter() :-(
    for i, (x, y) in enumerate(points):
        ax.scatter(
            x,
            y,
            marker=f"${i+1}$",
            c=color,
            label=label,
            zorder=3,
            transform=CRS_EQUATORIAL,
        )


def plot_aspect_points(
    ax: Axes,
    fixes: AspectFixes,
    masks: list[NDArray[np.bool]] | None
) -> None:
    if masks is None:
        ax.scatter(
            fixes.ra,
            fixes.dec,
            marker="o",
            c="#636363",
            linewidths=0,
            edgecolors="none",
            transform=CRS_EQUATORIAL,
        )
    else:
        from colorcet import b_glasbey_bw_minc_20_maxl_70 as CLUST_COLORS
        for clust, mask in enumerate(masks):
            color = CLUST_COLORS[clust % len(CLUST_COLORS)]
            ax.scatter(
                fixes.ra[mask],
                fixes.dec[mask],
                marker="o",
                c=color,
                linewidths=0,
                edgecolors="none",
                transform=CRS_EQUATORIAL,
            )


def plot_apertures(
    ax: Axes,
    legs: LegAssignment,
) -> None:
    from colorcet import b_glasbey_bw_minc_20_hue_150_280 as NORM_COLORS
    from colorcet import b_glasbey_bw_minc_20_hue_330_100 as DEVIANT_COLORS

    for leg, (metric, (full, partial)) in enumerate(zip(
        legs.leg_metrics, legs.leg_regions
    )):

        if metric.is_acceptable():
            fill = NORM_COLORS[leg % len(NORM_COLORS)]
        else:
            fill = DEVIANT_COLORS[leg % len(DEVIANT_COLORS)]

        if not partial.is_empty:
            ax.add_geometries(
                [partial],
                PC_EQUATORIAL,
                edgecolor="none",
                facecolor=fill,
                alpha=0.2,
            )
        if not full.is_empty:
            ax.add_geometries(
                [full],
                PC_EQUATORIAL,
                edgecolor="none",
                facecolor=fill,
                alpha=0.4,
            )

    autolim_with_geoms(ax)


def plot_diagnostic(
    diag_dir: Path,
    ec: Eclipse,
    planned_tgts: NDArray[np.float64],
    fixes: AspectFixes,
    legs: LegAssignment
) -> None:
    from matplotlib import pyplot as plt

    # this should be a savefig option, not a global
    plt.rcParams["svg.fonttype"] = "none"

    if ec.metadata.ra0 is None or ec.metadata.dec0 is None:
        return

    plot_proj = plane_projection(ec.metadata.ra0, ec.metadata.dec0)
    observed_tgts = np.array([[b.ra0, b.dec0] for b in ec.boresight])

    fig = plt.figure(figsize = (2 * PANEL_WIDTH, 2 * PANEL_WIDTH))
    try:
        ax_runs = fig.add_subplot(2, 2, 1, projection=plot_proj)
        ax_grps = fig.add_subplot(2, 2, 2, projection=plot_proj)
        ax_legs = fig.add_subplot(2, 2, 3, projection=plot_proj)
        ax_aprs = fig.add_subplot(2, 2, 4, projection=plot_proj)

        ax_runs.set_title("runs")
        plot_aspect_points(ax_runs, fixes, legs.run_masks)

        ax_grps.set_title("groups")
        plot_aspect_points(ax_grps, fixes, legs.grp_masks)

        ax_legs.set_title("legs")
        plot_targets(ax_legs, planned_tgts, "#969696", "planned")
        plot_targets(ax_legs, observed_tgts, "#377eb8", "observed")
        plot_aspect_points(ax_legs, fixes, legs.leg_masks)

        ax_aprs.set_title("aperture")
        plot_targets(ax_aprs, observed_tgts, "#ffffff", "observed")
        plot_apertures(ax_aprs, legs)

        for ax in [ax_runs, ax_grps, ax_legs, ax_aprs]:
            ax.set_aspect("equal")
            gl = ax.gridlines(draw_labels=True, dms=True,
                              x_inline=False, y_inline=False)
            gl.top_labels = False
            gl.right_labels = False

        n_tgts = planned_tgts.shape[0]
        fig.suptitle(
            f"{ec.metadata.eclipse}"
            f" ({ec.metadata.plan_type}/{ec.metadata.plan_subtype});"
            f" {n_tgts} target{'s' if n_tgts != 1 else ''},"
            f" {legs.n_legs} leg{'s' if legs.n_legs != 1 else ''},"
            f" {fixes.n_runs} run{'s' if fixes.n_runs != 1 else ''}"
        )
        plt.savefig(diag_dir / f"{ec.metadata.eclipse}.svg")
    finally:
        plt.close(fig)


def aspect_for_eclipse(aspect_dir: Path, eclipse: int) -> DataFrame:
    return parquet.read_table(
        aspect_dir / "aspect.parquet",
        filters = [("eclipse", "==", eclipse)],
        columns = ["time", "flags", "ra", "dec"],
        use_threads = False,
    ).to_pandas()


def crunch_eclipse_1(
    de: DownloadedEclipse,
    aspect_dir: Path,
    diag_dir: Path | None,
    diag_all: bool
) -> Eclipse:
    scst_metadata = metadata_from_scst(de)
    aspect = aspect_for_eclipse(aspect_dir, de.eclipse)
    if len(aspect) == 0:
        return no_aspect_eclipse(scst_metadata)

    planned_tgts = planned_targets(scst_metadata)
    fixes = ok_aspect_fixes(aspect)
    legs = choose_legs(fixes, de.eclipse, diag_dir)

    new_recs = finish_eclipse(
        scst_metadata,
        planned_tgts,
        fixes,
        legs,
    )

    if (not legs.is_acceptable() or diag_all) and diag_dir is not None:
        diag_subdir = diag_dir / new_recs.category
        diag_subdir.mkdir(exist_ok=True)

        plot_diagnostic(
            diag_subdir,
            new_recs,
            planned_tgts,
            fixes,
            legs,
        )

    return new_recs


def crunch_eclipse(
    de: DownloadedEclipse,
    aspect_dir: Path,
    diag_dir: Path | None,
    diag_all: bool,
) -> Eclipse:
    try:
        return crunch_eclipse_1(de, aspect_dir, diag_dir, diag_all)
    except Exception as e:
        t = type(e).__name__
        m = str(e)
        if t == m:
            msg = m
        elif m == "":
            msg = t
        else:
            msg = f"{t}: {m}"
        raise EclipseCrunchError(de.eclipse, msg) from e


def load_scst_index(scst_dir: Path) -> dict[int, DownloadedEclipse]:
    # {scst_dir}/index.shelf includes records for all eclipse numbers
    # in the range [0, 49999], which is a substantial superset of the
    # eclipses for which there is actually any data.
    def gen_downloaded_eclipses(shelf: shelve.Shelf[Any]) -> Iterable[tuple[int, DownloadedEclipse]]:
        for k, v in shelf.items():
            scst_fname = v["scst"]
            if scst_fname is not None:
                eclipse = int(k)
                yield (eclipse, DownloadedEclipse(
                    eclipse = eclipse,
                    scst_file = scst_dir / scst_fname,
                    has_aspect = v["gphoton_aspect"],
                    has_nuv_raw6 = v["nuv_raw6"],
                    has_fuv_raw6 = v["fuv_raw6"],
                ))

    with shelve.open(scst_dir / "index.shelf", "r") as shelf:
        return dict(gen_downloaded_eclipses(shelf))


#
# Output
#

def arrow_type_for_py_type(t: type) -> pa.DataType:
    if t is int:
        return pa.uint64()
    if t is float:
        return pa.float64()
    if t is bool:
        return pa.bool_()
    if t is str:
        return pa.string()
    if t is bytes:
        return pa.binary()
    if isinstance(t, UnionType):
        if len(t.__args__) == 2:
            if t.__args__[0] is NoneType:
                return arrow_type_for_py_type(t.__args__[1])
            if t.__args__[1] is NoneType:
                return arrow_type_for_py_type(t.__args__[0])

    raise ValueError(f"don't know Arrow equiv for {t!r}")


class ColumnSponge:
    """Absorb a sequence of rows of a particular record type and
       convert them to a column-oriented pyarrow Table."""
    def __init__(self, dc: type):
        columns: dict[str, list[tuple[str, Any]]] = {}
        fields = []
        for f in dc_fields(dc):
            columns[f.name] = []
            fields.append((f.name, arrow_type_for_py_type(f.type)))
        self.columns = columns
        self.schema = pa.schema(fields)

    def append(self, record: Any) -> None:
        for nm in self.schema.names:
            self.columns[nm].append(getattr(record, nm))

    def extend(self, records: Iterable[Any]) -> None:
        for rec in records:
            self.append(rec)

    def flush(self) -> pa.Table:
        columns = []
        for nm in self.schema.names:
            arr = pa.array(self.columns[nm], self.schema.field(nm).type)
            columns.append(arr)

        return pa.Table.from_arrays(columns, schema=self.schema)


class NewTableWriter:
    def __init__(self, output_dir: Path):
        self.metadata  = output_dir / "metadata.parquet"
        self.boresight = output_dir / "boresight.parquet"
        self.leg_aper  = output_dir / "leg-aperture.parquet"

        self.md_records  = ColumnSponge(MetadataRecord)
        self.bst_records = ColumnSponge(BoresightRecord)
        self.la_records  = ColumnSponge(LegApertureRecord)

    def __enter__(self) -> Self:
        return self

    def __exit__(self, ty: None | type, _val: Any, _tb: Any) -> None:
        # only write out the tables on normal exit
        if ty is None:
            self.flush()

    def add(self, rec: Eclipse) -> None:
        self.md_records.append(rec.metadata)
        self.bst_records.extend(rec.boresight)
        self.la_records.extend(rec.apertures)

    def flush(self) -> None:
        progress("compiling tables...")
        md  = self.md_records.flush()
        bst = self.bst_records.flush()
        la  = self.la_records.flush()

        md = md.sort_by([("eclipse", "ascending")])
        bst = bst.sort_by([("eclipse", "ascending"), ("leg", "ascending")])
        la = la.sort_by([("eclipse", "ascending"), ("leg", "ascending")])

        progress("writing tables...")
        parquet.write_table(md, self.metadata)
        parquet.write_table(bst, self.boresight)
        # this one has really big blobs and is known not to benefit from
        # dictionary encoding
        parquet.write_table(la, self.leg_aper,
                            use_dictionary=False, compression="zstd")
        progress("done.")


def process_update(total: int, done: int, acc: int, dev: int, err: int,
                   status: dict[str, int]) -> None:
    breakdown = " ".join(
        str(status[cat]) + LegAssignment.CATEGORY_SHORT_NAMES[cat]
        for cat in LegAssignment.ALL_CATEGORIES
    )
    progress(f"{done}/{total}: {breakdown}; {acc}a {dev}d {err}e")


# This is a workaround for ProcessPoolExecutor trying to be too clever.
# See below.
def noop() -> None:
    pass


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("aspect_dir", type=Path,
                    help="find aspect tables in this directory")
    ap.add_argument("scst_dir", type=Path,
                    help="find indexed SCST files in this directory")
    ap.add_argument("output_dir", type=Path,
                    help="new tables will be written to this directory")
    ap.add_argument("eclipse", nargs="*", type=int,
                    help="process only these eclipses")
    ap.add_argument("--diag-images", type=Path, default=None,
                    help="write diagnostic images for hard eclipses"
                    " to this directory")
    ap.add_argument("--diag-all", action="store_true",
                    help="write diagnostic images for all eclipses")
    ap.add_argument("--debug", action="store_true",
                    help="print Python tracebacks for debugging")
    ap.add_argument("--stop-on-error", action="store_true",
                    help="stop on first error")
    args = ap.parse_args()

    # both of these directories must already exist
    args.aspect_dir = args.aspect_dir.resolve(strict=True)
    args.scst_dir = args.scst_dir.resolve(strict=True)

    # if these directories don't already exist, create them
    args.output_dir = args.output_dir.resolve(strict=False)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    if args.diag_images is not None:
        args.diag_images = args.diag_images.resolve(strict=False)
        if args.diag_images.exists():
            shutil.rmtree(args.diag_images)
        args.diag_images.mkdir(parents=True, exist_ok=False)

    if args.diag_all and args.diag_images is None:
        ap.error("must have --diag-images with --diag-all")

    if args.eclipse:
        args.eclipse = set(args.eclipse)
    else:
        args.eclipse = None

    np.seterr(all="raise")

    # We need to fork the worker processes _before_ loading tables,
    # so that only the parent process allocates memory for the tables,
    # otherwise we'll run the computer out of RAM and crash.
    # In order to force ProcessPoolExecutor to fork all the workers at
    # the right time, we have to specifically request the "fork" mode
    # of the underlying multiprocessing library, and we also have to
    # submit a no-op job to the pool immediately after creating it.
    ctx = multiprocessing.get_context("fork")
    with (
        ProcessPoolExecutor(mp_context=ctx) as pool,
        NewTableWriter(args.output_dir) as wr,
        open(args.output_dir / "ambig-eclipses.txt", "wt") as amb_log
    ):
        pool.submit(noop).result()

        progress("loading tables")
        scst_index = load_scst_index(args.scst_dir)

        progress("distributing eclipses...")
        futures = []
        try:
            for eclipse in scst_index.values():
                if (args.eclipse is not None
                    and eclipse.eclipse not in args.eclipse):
                    continue
                if len(futures) > 0 and len(futures) % 1000 == 0:
                    progress(f"{len(futures)} eclipses distributed")
                futures.append(pool.submit(
                    crunch_eclipse,
                    eclipse,
                    args.aspect_dir,
                    args.diag_images,
                    args.diag_all,
                ))
            total = len(futures)
            if total > 0 and total % 1000 != 0:
                progress(f"{total} eclipses distributed")

            progress("processing eclipses...")

            done = 0
            acc = 0
            dev = 0
            err = 0
            status = { cat: 0 for cat in LegAssignment.ALL_CATEGORIES }
            for fut in as_completed(futures):
                try:
                    result = fut.result()
                    wr.add(result)

                    done += 1
                    status[result.category] += 1

                    if not result.acceptable:
                        dev += 1
                        amb_log.write(f"{result.metadata.eclipse},deviant\n")
                        process_update(total, done, acc, dev, err, status)
                    else:
                        acc += 1
                        if done % 1000 == 0:
                            process_update(total, done, acc, dev, err, status)

                except EclipseCrunchError as e:
                    amb_log.write(f"{e.args[0]},error\n")
                    sys.stderr.write(str(e) + "\n")
                    if args.debug:
                        e.debug_report()
                    if args.stop_on_error:
                        sys.exit(1)

                    done += 1
                    err += 1
                    process_update(total, done, acc, dev, err, status)

        except BaseException:
            for f in futures:
                f.cancel()
            raise

        process_update(total, done, acc, dev, err, status)


main()
