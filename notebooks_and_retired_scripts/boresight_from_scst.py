import argparse
import csv
import itertools
import multiprocessing
import os
import shelve
import shutil
import sys
import time

from collections import namedtuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, fields as dc_fields
from datetime import datetime, timedelta, timezone
from itertools import combinations
from math import isnan, inf as Inf, nan as NaN, pi as PI
from pathlib import Path
from typing import Any, Iterable, Self

import numpy as np
import pyarrow as pa
import shapely

from astropy.io import fits
from gPhoton.constants import DETSIZE, PIXEL_SIZE
from gPhoton.coords.gnomonic import gnomfwd_simple, gnomrev_simple
from gPhoton.io import mast
from pandas import DataFrame
from pyarrow import parquet
from scipy.ndimage import label


# hardcoded cdelt and cenpix parameters from load_aspect_solution
CDELT  = 1.0 / 36000.0
CENPIX = 0.0

# conversion from distance in cdelt units to distance in arcsec
CDELT_TO_ARCSEC = 0.1

# conversion from area in square cdelt pixels to area in square arcsec
CDELT2_TO_ARCSEC2 = 0.01

# conversion from area in square cdelt pixels to area in square degrees
CDELT2_TO_DEG2 = CDELT2_TO_ARCSEC2 / (3600 * 3600)

# detector aperture radius in xi/eta units
APERTURE_RADIUS = (DETSIZE / CDELT) / 2

# Shapely has no concept of arcs; curves must be approximated using a
# sequence of lines.  The polygon generated by Point(x,y).buffer(r) is
# guaranteed to be inscribed within a true circle of radius r.  The
# approximation can be made more precise by increasing this parameter,
# which is the number of segments to use for each quarter-circle.
#
# At QUAD_SEGS = 72, the approximation of a circle with radius
# APERTURE_RADIUS has area 1262 cdelt^2 units smaller than a true
# circle with that radius; this is a relative error of 0.008%,
# which we should be able to live with.
QUAD_SEGS = 72

# The difference between the GPS epoch (1980-01-06T00:00:00Z) and the
# Unix epoch (1970-01-01T00:00:00Z).  SCST and RAW6 files both contain
# numeric timestamps counting from the GPS epoch.
GPS_EPOCH = datetime(1980, 1, 6, 0, 0, 0, 0, timezone.utc).timestamp()

# If the uncircularity of a fully exposed region is larger than this,
# the exposure needs to be split into multiple legs.
UNCIRCULARITY_LIMIT = 0.0075

# If the ratio of the area of the partially exposed region to the area
# of the fully exposed region is greater than this, the exposure needs
# to be split into multiple legs.
#
# The partially exposed region can get quite large without its
# actually being a problem, notably for petal-pattern eclipses, and
# the uncircularity limit catches most of the problem cases we care
# about; this mainly exists to deal with the petal-pattern eclipses
# where the pattern kept its radial symmetry but became abnormally
# large.
AREA_RATIO_LIMIT = 0.75

# There are very few legs this short and they usually consist of just
# one or two points that should have been flagged but weren't.
MIN_LEG_EXPOSURE_TIME = 15

# Size of a diagnostic panel
PANEL_WIDTH = 4

# 12-class Brewer Set3
LEG_COLORS = [
    '#8dd3c7',
    '#ffffb3',
    '#bebada',
    '#fb8072',
    '#80b1d3',
    '#fdb462',
    '#b3de69',
    '#fccde5',
    '#d9d9d9',
    '#bc80bd',
    '#ccebc5',
    '#ffed6f'
]


START_TIME = None
def progress(msg):
    now = time.monotonic()

    global START_TIME
    if START_TIME is None:
        START_TIME = now

    elapsed = timedelta(seconds=now - START_TIME)
    sys.stderr.write(f"[{elapsed}]  {msg}\n")


@dataclass(frozen=True, slots=True)
class DownloadedEclipse:
    eclipse: int
    scst_file: Path
    nuv_has_raw6: bool
    fuv_has_raw6: bool


@dataclass(frozen=True, slots=True)
class SCSTMetadata:
    eclipse: int
    visit: int
    plan_type: str
    plan_subtype: str
    plan_id: int
    planned_legs: int

    eclipse_start: float
    eclipse_duration: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_has_raw6: bool

    roll: float
    legs: np.ndarray


@dataclass(frozen=True, slots=True)
class AspectFixes:
    time: np.ndarray
    run_id: np.ndarray
    ra: np.ndarray
    dec: np.ndarray
    roll: np.ndarray
    n_runs: int


@dataclass(frozen=True, slots=True)
class ExposureMetric:
    full_area: float
    partial_area_ratio: float
    uncircularity: float

    def is_acceptable(self) -> bool:
        return (
            self.full_area > 0
            and self.partial_area_ratio <= AREA_RATIO_LIMIT
            and self.uncircularity <= UNCIRCULARITY_LIMIT
        )

    def __str__(self):
        return (
            f"[fa: {self.full_area:g}"
            f" pa_ratio: {self.partial_area_ratio:.3f}"
            f" uncirc: {self.uncircularity:.4g}]"
        )

@dataclass(frozen=True, slots=True)
class LegAssignment:
    leg_ids: np.ndarray
    n_legs: int

@dataclass(slots=True, frozen=True)
class MetadataRecord:
    eclipse: int
    plan_type: str
    plan_subtype: str
    visit: int
    plan_id: int
    planned_legs: int
    observed_legs: int

    eclipse_start: float
    eclipse_duration: float
    ok_exposure_time: float

    ra_min: float
    ra_max: float

    dec_min: float
    dec_max: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_has_raw6: bool


@dataclass(slots=True, frozen=True)
class BoresightRecord:
    eclipse: int
    leg: int
    time: float
    duration: float

    ra0: float
    dec0: float
    roll0: float

    ra_min: float
    ra_max: float
    dec_min: float
    dec_max: float

    planned_ra: float
    planned_dec: float

    full_exposure_area: float
    full_exposure_uncircularity: float
    partial_exposure_area_ratio: float


@dataclass(slots=True, frozen=True)
class LegApertureRecord:
    eclipse: int
    leg: int

    full_exposure_region: bytes
    partial_exposure_region: bytes


@dataclass(slots=True, frozen=True)
class Eclipse:
    metadata: MetadataRecord
    boresight: list[BoresightRecord]
    apertures: list[LegApertureRecord]
    deviant: bool


class EclipseCrunchError(RuntimeError):
    def debug_report(self):
        if hasattr(self.__cause__, "tb"):
            skipping = True
            for line in self.__cause__.tb.splitlines():
                if skipping:
                    if line == '"""':
                        skipping = False
                else:
                    if line == ("The above exception was the direct cause"
                                " of the following exception:"):
                        break
                    sys.stderr.write(line + "\n")
        else:
            import traceback
            traceback.print_exc(self.__cause__)


def scst_timestamp(stamp: str) -> float:
    """Parse a string timestamp from the format found in SCST file
       headers.  Returns a floating-point count of seconds since the GPS
       epoch.  (This epoch makes them directly comparable to the
       *numeric* timestamps found in the SCST file's *data.*)"""
    return datetime.strptime(stamp, "%y%m%dT%H%M%S%z").timestamp() - GPS_EPOCH


def metadata_from_scst(de: DownloadedEclipse) -> SCSTMetadata:
    """Extract all of the per-eclipse metadata that we care about from
       an SCST file."""
    with fits.open(de.scst_file) as hdulist:
        header = hdulist[0].header
        eclipse = header["ECLIPSE"]

        # For whatever reason, the numeric timestamps in an SCST
        # file's actual data are almost always nnnnnn.995 -- that is,
        # 5000 μs *before* a whole number of seconds since the epoch.
        # The eclipse start time, however, is expressed as a string
        # timestamp with no subsecond information.  Fudge the start
        # time 0.01 s (10,000 μs) earlier to ensure that it will
        # compare strictly less than any data timestamp.
        eclipse_start = scst_timestamp(header["TRANGE0"]) - 0.01

        nuv_det_on_time = header.get('NHVNOMN', 0)
        fuv_det_on_time = header.get('NHVNOMF', 0)
        nuv_det_temp    = header.get('NDTDET', NaN)
        nuv_tdc_temp    = header.get('NDTTDC', NaN)
        fuv_det_temp    = header.get('FDTDET', NaN)
        fuv_tdc_temp    = header.get('FDTTDC', NaN)

        ra0  = header["RA_CENT"]
        dec0 = header["DEC_CENT"]
        ra1  = header.get("RA_1")
        dec1 = header.get("DEC_1")
        if ra1 is not None and ra1 != ra0:
            raise AssertionError(
                f"e{eclipse}: RA_CENT = {ra0} != {ra1} = RA_1")
        if dec1 is not None and dec1 != dec0:
            raise AssertionError(
                f"e{eclipse}: DEC_CENT = {dec0} != {dec1} = DEC_1")

        legs = [ (ra0, dec0) ]
        planned_legs = max(1, header["MPSNPOS"])
        if planned_legs >= 2:
            for i in range(2, planned_legs + 1):
                legs.append((header.get(f"RA_{i}"), header.get(f"DEC_{i}")))

        return SCSTMetadata(
            eclipse            = eclipse,
            visit              = header["VISIT"],
            plan_type          = header["MPSTYPE"],
            plan_subtype       = header["MPSPLAN"],
            plan_id            = header["PLANID"],
            planned_legs       = planned_legs,

            # As the stop time is also expressed as a string timestamp,
            # it will compare greater than any data timestamp from this
            # eclipse with no need for any fudging.
            eclipse_start      = eclipse_start,
            eclipse_duration   = (scst_timestamp(header["TRANGE1"])
                                  - eclipse_start),

            nuv_det_on_time    = nuv_det_on_time,
            nuv_det_temp       = nuv_det_temp,
            nuv_tdc_temp       = nuv_tdc_temp,
            nuv_has_raw6       = de.nuv_has_raw6,

            fuv_det_on_time    = fuv_det_on_time,
            fuv_det_temp       = fuv_det_temp,
            fuv_tdc_temp       = fuv_tdc_temp,
            fuv_has_raw6       = de.fuv_has_raw6,

            roll               = header["ROLL"],
            legs               = np.array(legs)
        )


def transform_to_xi_eta(
    ra0: float,
    dec0: float,
    ra: np.ndarray,
    dec: np.ndarray,
    roll: np.ndarray
) -> (np.ndarray, np.ndarray):
    """convert a sequence of equatorial sky coordinates to detector
       coordinates, taking (ra0, dec0) as the sky coordinate corresponding
       to detector (0,0)."""
    ra, dec, roll = np.broadcast_arrays(ra, dec, roll)
    return gnomfwd_simple(
        ra,
        dec,
        ra0,
        dec0,
        -roll,
        CDELT,
        CENPIX
    )


def transform_to_ra_dec(
    ra0: float,
    dec0: float,
    xi: np.ndarray,
    eta: np.ndarray,
) -> (np.ndarray, np.ndarray):
    """convert a sequence of detector coordinates to equatorial sky
       coordinates, taking (ra0, dec0) as the sky coordinate corresponding
       to detector (0,0)."""
    return gnomrev_simple(
        xi,
        eta,
        ra0,
        dec0,
        0,
        CDELT,
        CENPIX,
        CENPIX
    )

def point_to_ra_dec(
    ra0: float,
    dec0: float,
    xi: float,
    eta: float,
) -> (float, float):
    ra, dec = transform_to_ra_dec(ra0, dec0, np.array([xi]), np.array([eta]))
    return ra.item(), dec.item()


def ok_aspect_fixes(aspect: DataFrame) -> AspectFixes:
    """extract the time of each aspect fix from the aspect table,
       calculate whether each aspect fix is considered part of the
       observation, and return (time, run_id, ra, dec, roll) for each
       fix that is considered part of the observation."""
    # if the mysterious 'flags' number is odd, it means this aspect
    # record should not be considered part of the observation.
    # the full set of reasons for flagging an aspect record is not
    # known, but the most common reasons are that the detector hadn't
    # yet come up to operating voltage, or that the telescope was
    # carrying out a high-speed slew during that time step.
    ok = aspect["flags"].to_numpy() & 1 == 0

    # we also distrust aspect records immediately before and after a
    # flagged aspect record; the first record in 'aspect' is assumed
    # to be preceded by a flagged record, and the last record is
    # assumed to be followed by a flagged record
    ok &= np.concat([[False], ok[:-1]])
    ok &= np.concat([ok[1:], [False]])

    # we also distrust aspect records whose time step is not 1
    times = aspect["time"].to_numpy()
    ok[1:] &= np.isclose(times[1:] - times[:-1], 1.0)

    # give each run of unflagged fixes a serial number
    runs, n_runs = label(ok)

    return AspectFixes(
        time   = times[ok],
        # label() used 0 for flagged points and [1, n_runs] for
        # unflagged points; we're throwing away all the unflagged
        # points, so shift the labels to [0, n_runs), aligned with
        # what you get from range(n_runs).
        run_id = runs[ok] - 1,
        ra     = aspect["ra"].to_numpy()[ok],
        dec    = aspect["dec"].to_numpy()[ok],
        roll   = aspect["roll"].to_numpy()[ok],
        n_runs = n_runs
    )


def aperture_disks(xi, eta):
    """For each point (xi, eta) construct a circle centered at that
    point whose radius is the radius of the GALEX detector aperture.
    """
    return shapely.buffer(
        shapely.points(xi, eta),
        APERTURE_RADIUS,
        quad_segs=QUAD_SEGS,
    )


def exposure_for_apertures(disks):
    """Given a set of disks, each corresponding to the GALEX detector
    aperture at some time step, produce the region, on an imaginary
    detector plate of unbounded extent, that is common to all the
    disks (which is the region that was exposed to incoming light
    during the entire exposure) and the region that is common to some
    but not all the disks (exposed to light during some but not all of
    the exposure).
    """
    full = shapely.intersection_all(disks)
    partial = shapely.difference(shapely.union_all(disks), full)

    return full, partial


def exposure_goodness(full, partial) -> ExposureMetric:
    """Compute measures of how stable the telescope seems to
    have been during an exposure characterized by the regions
    FULL and PARTIAL.

    The return value has 3 attributes:
     - full_area: Area of the full exposure region.  Note that this
       is in detector space coordinates (xi, eta) and the transformation
       from detector to sky coordinates is NOT area-preserving.
     - partial_area_ratio: Ratio of the area of the partially exposed
       region to the area of the full exposure region.
     - uncircularity: Measure of how far off the full region is from
       a perfect circle.  Specifically, this is the reciprocal
       isoperimetric quotient minus one; zero is a perfect circle.
       See <https://en.wikipedia.org/wiki/Isoperimetric_inequality>.
       Using the reciprocal quotient and subtracting one facilitates
       plotting typical-for-us values on a log scale.
    """

    # for AIS eclipses there might be no overlap at all when
    # considering the entire eclipse at once
    if full.is_empty:
        return ExposureMetric(
            full_area = 0,
            partial_area_ratio = Inf,
            uncircularity = Inf,
        )

    # I don't trust shapely not to recalculate these properties on
    # every access.
    full_perimeter = full.exterior.length
    full_area = full.area

    # The standard isoperimetric quotient is 4πA⁄P², which ranges from
    # 0 to 1, where 1 is a perfect circle and 0 is a shape with zero
    # area but nonzero perimeter, e.g. a line segment.  Its reciprocal
    # ranges from 1 to positive infinity, and subtracting 1 puts a
    # perfect circle at 0.  We do this transformation because most
    # legs have an isoperimetric quotient very close to 1; on our data
    # set, the shifted reciprocal quotient ranges from 4 × 10⁻⁵ to
    # 0.21, which is usefully put on a log scale.)

    s_r_isop = full_perimeter * full_perimeter / (4 * np.pi * full_area) - 1

    return ExposureMetric(
        full_area = full_area,
        partial_area_ratio = partial.area / full_area,
        uncircularity = s_r_isop
    )

def goodness_for_apertures(disks):
    return exposure_goodness(*exposure_for_apertures(disks))


def legs_by_circularity(fixes: AspectFixes) -> LegAssignment:
    """Merge runs of aspect fixes from 'fixes' into legs.

    A leg is one or more _consecutive_ runs which produce
    a full exposure region whose uncircularity is less than
    UNCIRCULARITY_LIMIT, and which produce a partial exposure
    region whose size is less than AREA_RATIO_LIMIT * (size of
    full exposure region).
    """

    # Arbitrary choice of detector center for gnomonic projection.
    # The shapes will be recalculated later with more precisely
    # chosen detector centers, and this only affects numerical
    # error anyway.
    ra0 = fixes.ra[len(fixes.ra)//2]
    dec0 = fixes.dec[len(fixes.dec)//2]
    xi, eta = transform_to_xi_eta(ra0, dec0, fixes.ra, fixes.dec, fixes.roll)
    disks = aperture_disks(xi, eta)

    # First try a single leg for the entire eclipse.
    full_depth_metric = goodness_for_apertures(disks)
    if full_depth_metric.is_acceptable():
        return LegAssignment(
            leg_ids = np.zeros(fixes.ra.shape, np.int32),
            n_legs = 1
        )

    # Each leg must be one or more _consecutive_ runs.
    # We want to find the set of splits that simultaneously minimizes
    # the number of legs, the uncircularity of each leg, and the partial
    # area ratio of each leg.
    # TODO: The algorithm below is greedy and satisficing.  It does not
    # always find the optimum.  Replace with dynamic programming.
    leg_ids = np.full(fixes.ra.shape, -1, np.int32)
    n_legs = 0

    leg_mask = np.full(fixes.ra.shape, False, np.bool_)
    for run in range(fixes.n_runs):
        run_mask = fixes.run_id == run
        trial_leg_mask = leg_mask | run_mask
        metric = goodness_for_apertures(disks[run_mask])
        if not metric.is_acceptable():
            # adding the current run to the leg made it too warped
            if not leg_mask.any():
                # the current run is already too warped
                raise AssertionError(
                    f"single run too warped: {metric=}"
                )

            leg_ids[leg_mask] = n_legs
            n_legs += 1
            leg_mask = run_mask
        else:
            leg_mask = trial_leg_mask

    # don't lose the very last leg
    if leg_mask.any():
        leg_ids[leg_mask] = n_legs
        n_legs += 1

    return LegAssignment(
        leg_ids = leg_ids,
        n_legs = n_legs,
    )


def planned_targets(scst_metadata):
    planned_ra0 = [c[0] for c in scst_metadata.legs]
    planned_dec0 = [c[1] for c in scst_metadata.legs]
    return np.array([planned_ra0, planned_dec0]).transpose()


def boresight_for_leg(
    eclipse: int,
    leg_index: int,
    fixes: AspectFixes,
    planned_tgts: np.ndarray,
    legs: LegAssignment,
    original_aspect: DataFrame,
) -> (BoresightRecord, LegApertureRecord):

    this_leg = legs.leg_ids == leg_index
    time = fixes.time[this_leg]
    ra = fixes.ra[this_leg]
    dec = fixes.dec[this_leg]
    roll = fixes.roll[this_leg]

    if len(ra) == 0:
        raise AssertionError(
            f"{eclipse}/{leg_index}: empty leg; {legs.leg_ids=}"
        )

    # Iteratively compute the centroid of the leg, which we will
    # use as the observed detector center of the leg, by repeatedly
    # projecting the aperture points into detector coordinates,
    # computing the full-exposure region and its centroid,
    # back-projecting that centroid into sky coordinates, and using
    # the result as the projection center for the next iteration,
    # until the values converge.  Again the starting point is arbitrary.
    #
    # It would probably be better to compute the centroid of the point
    # set in spherical coordinates (see e.g.
    # <https://skeptric.com/calculate-centroid-on-sphere/> but that's
    # a mess and I'm not sure how to take roll into account.

    ra0 = ra[len(ra)//2]
    dec0 = dec[len(dec)//2]
    ra0_new = None
    dec0_new = None
    iterations = 0
    while True:
        xi, eta = transform_to_xi_eta(ra0, dec0, ra, dec, roll)
        ap_disks = aperture_disks(xi, eta)
        full, partial = exposure_for_apertures(ap_disks)
        if full.is_empty:
            raise AssertionError(
                f"{eclipse}/{leg_index}: empty regions {ra0=} {ra0_new=} {dec0=} {dec0_new=}"
        )
        centroid = full.centroid
        ra0_new, dec0_new = point_to_ra_dec(ra0, dec0, centroid.x, centroid.y)
        if np.isclose(ra0, ra0_new) and np.isclose(dec0, dec0_new):
            break
        iterations += 1
        if iterations >= 20:
            raise RuntimeError(
                f"not converging - {ra0=} {ra0_new=} {dec0=} {dec0_new=}"
            )

    # partial can be empty if the leg is a single point
    if not partial.is_empty:
        bbox = partial.bounds
    else:
        assert not full.is_empty
        bbox = full.bounds

    metric = exposure_goodness(full, partial)

    # convert bbox and centroid back to sky coordinates
    ra_bounds, dec_bounds = transform_to_ra_dec(
        ra0, dec0,
        np.array([bbox[0], bbox[2]]),
        np.array([bbox[1], bbox[3]])
    )

    # do we have a planned target for this leg?
    if leg_index < planned_tgts.shape[0]:
        planned_ra, planned_dec = planned_tgts[leg_index, :]
    else:
        planned_ra = None
        planned_dec = None

    # If possible, expand the time range to ensure that there is one
    # flagged aspect point on either end of the run.  This ensures
    # that gphoton2 doesn't get confused by time ranges that consist
    # _only_ of unflagged points.
    original_time = original_aspect["time"].to_numpy()
    original_flagged = original_aspect["flags"].to_numpy() & 1 != 0

    start_ix = 0
    while original_time[start_ix] < time[0]:
        start_ix += 1
    while start_ix > 0 and not original_flagged[start_ix]:
        start_ix -= 1

    limit = len(original_time) - 1
    stop_ix = limit
    while original_time[stop_ix] > time[-1]:
        stop_ix -= 1
    while stop_ix < limit and not original_flagged[stop_ix]:
        stop_ix += 1

    # Aspect time points all end in .995 for some reason.  Use this to
    # make the boresight start and end times be strictly less and
    # greater than the time points at either end of the range.
    start_time = original_time[start_ix] - 0.005   # nnnn.990
    stop_time = original_time[stop_ix] + 0.005     # nnnm.000
    duration = stop_time + 1 - start_time

    leg_id = leg_index + 1

    return (
        BoresightRecord(
            eclipse                     = eclipse,
            leg                         = leg_id,
            time                        = start_time,
            duration                    = duration,
            ra0                         = ra0,
            dec0                        = dec0,
            roll0                       = 0.0,
            planned_ra                  = planned_ra,
            planned_dec                 = planned_dec,
            ra_min                      = ra_bounds[0],
            ra_max                      = ra_bounds[2],
            dec_min                     = dec_bounds[0],
            dec_max                     = dec_bounds[2],
            full_exposure_area          = metric.full_area,
            full_exposure_uncircularity = metric.uncircularity,
            partial_exposure_area_ratio = metric.partial_area_ratio,
        ),
        LegApertureRecord(
            eclipse                 = eclipse,
            leg                     = leg_id,
            full_exposure_region    = full.wkb,
            partial_exposure_region = partial.wkb,
        )
    )


def finish_eclipse(
    md: SCSTMetadata,
    planned_tgts: np.ndarray,
    fixes: AspectFixes,
    legs: LegAssignment,
    original_aspect: DataFrame,
) -> Eclipse:
    boresight_recs = []
    la_recs = []
    deviant = legs.n_legs != planned_tgts.shape[0]

    for leg in range(legs.n_legs):
        br, la = boresight_for_leg(
            md.eclipse, leg,
            fixes,
            planned_tgts,
            legs,
            original_aspect,
        )
        boresight_recs.append(br)
        la_recs.append(la)
        if (
            br.full_exposure_uncircularity > UNCIRCULARITY_LIMIT
            or br.partial_exposure_area_ratio > AREA_RATIO_LIMIT
        ):
            deviant = True

    ra_min = min(br.ra_min for br in boresight_recs)
    ra_max = max(br.ra_max for br in boresight_recs)
    dec_min = min(br.dec_min for br in boresight_recs)
    dec_max = max(br.dec_max for br in boresight_recs)

    return Eclipse(
        metadata=MetadataRecord(
            eclipse          = md.eclipse,
            visit            = md.visit,
            plan_type        = md.plan_type,
            plan_subtype     = md.plan_subtype,
            plan_id          = md.plan_id,
            planned_legs     = md.planned_legs,
            observed_legs    = legs.n_legs,

            eclipse_start    = md.eclipse_start,
            eclipse_duration = md.eclipse_duration,
            ok_exposure_time = len(fixes.time),

            nuv_det_on_time  = md.nuv_det_on_time,
            nuv_det_temp     = md.nuv_det_temp,
            nuv_tdc_temp     = md.nuv_tdc_temp,
            nuv_has_raw6     = md.nuv_has_raw6,

            fuv_det_on_time  = md.fuv_det_on_time,
            fuv_det_temp     = md.fuv_det_temp,
            fuv_tdc_temp     = md.fuv_tdc_temp,
            fuv_has_raw6     = md.fuv_has_raw6,

            ra_min           = ra_min,
            ra_max           = ra_max,
            dec_min          = dec_min,
            dec_max          = dec_max,
        ),
        boresight=boresight_recs,
        apertures=la_recs,
        deviant=deviant
    )


def plot_targets(ax, xi, eta, color, label):
    # can only apply one marker shape per call to scatter() :-(
    for i, (x, y) in enumerate(zip(xi, eta)):
        ax.scatter(x, y, marker=f"${i+1}$", c=color, label=label, zorder=3)


def plot_legs(ax, xi, eta, legs):
    for leg in range(legs.n_legs):
        leg_mask = legs.leg_ids == leg
        leg_color = LEG_COLORS[leg % len(LEG_COLORS)]
        ax.scatter(
            xi[leg_mask], eta[leg_mask], marker="o",
            c=leg_color, linewidths=0, edgecolors="none",
        )

def plot_aperture(ax, bore, rgn):
    from shapely.plotting import plot_polygon

    plot_polygon(
        polygon=shapely.from_wkb(rgn.partial_exposure_region),
        ax=ax,
        add_points=False,
        edgecolor="#d95f02",
        facecolor="#fc8d62"
    )

    mic = shapely.maximum_inscribed_circle(
        shapely.from_wkb(rgn.full_exposure_region)
    )
    plot_polygon(
        polygon=shapely.Point(mic.coords[0]).buffer(mic.length),
        ax=ax,
        add_points=False,
        edgecolor="#1b9e77", facecolor="#66c2a5"
    )

    ax.set_title(f"uncirc={bore.full_exposure_uncircularity:.4g}"
                 f" pa-ratio={bore.partial_exposure_area_ratio:.4f}")

def plot_diagnostic(
    diag_dir: Path,
    ec: Eclipse,
    planned_tgts: np.ndarray,
    fixes: AspectFixes,
    legs: LegAssignment
) -> None:
    from matplotlib import pyplot as plt

    # convert all sky coordinates to detector coordinates for plotting,
    # using the center of the eclipse's bounding box as the detector center
    if ec.metadata.ra_max < ec.metadata.ra_min:
        ra0 = (ec.metadata.ra_min + ec.metadata.ra_max + 360)/2
    else:
        ra0 = (ec.metadata.ra_min + ec.metadata.ra_max)/2
    dec0 = (ec.metadata.dec_min + ec.metadata.dec_max) / 2

    xi, eta = transform_to_xi_eta(
        ra0, dec0,
        fixes.ra, fixes.dec, fixes.roll
    )
    cent_xi, cent_eta = transform_to_xi_eta(
        ra0, dec0,
        np.array([b.ra0 for b in ec.boresight]),
        np.array([b.dec0 for b in ec.boresight]),
        0.0,
    )

    # planned targets are (meant to be) center of detector, so it
    # doesn't matter what roll we use for them
    planned_xi, planned_eta = transform_to_xi_eta(
        ra0, dec0,
        planned_tgts[:, 0],
        planned_tgts[:, 1],
        0.0
    )

    n_panels = len(ec.boresight) + 2
    n_rows = int(np.floor(np.sqrt(n_panels)).item())
    n_cols = int(np.ceil(n_panels / n_rows).item())

    fig, ax = plt.subplots(
        n_rows, n_cols,
        figsize=(n_cols * PANEL_WIDTH, n_rows * PANEL_WIDTH),
    )
    try:
        ax = ax.ravel()
        ax[0].set_aspect("equal", "box")

        plot_targets(ax[0], planned_xi, planned_eta, "#aaaaaa", "planned")
        plot_targets(ax[0], cent_xi, cent_eta, "#377eb8", "observed")
        plot_legs(ax[0], xi, eta, legs)

        plot_targets(ax[1], cent_xi, cent_eta, "#377eb8", "observed")
        plot_legs(ax[1], xi, eta, legs)

        for i in range(len(ec.boresight)):
            ax[i+2].set_aspect("equal", "box")
            plot_aperture(ax[i+2], ec.boresight[i], ec.apertures[i])

        for i in range(len(ec.boresight) + 2, n_rows * n_cols):
            ax[i].set_axis_off()

        n_tgts = len(planned_xi)
        fig.suptitle(
            f"{ec.metadata.eclipse}"
            f" ({ec.metadata.plan_type}/{ec.metadata.plan_subtype});"
            f" {n_tgts} target{'s' if n_tgts != 1 else ''},"
            f" {legs.n_legs} leg{'s' if legs.n_legs != 1 else ''},"
            f" {fixes.n_runs} run{'s' if fixes.n_runs != 1 else ''}"
        )
        plt.savefig(diag_dir / f"{ec.metadata.eclipse}.png")
    finally:
        plt.close(fig)


def crunch_eclipse_1(
    de: DownloadedEclipse,
    aspect: DataFrame,
    diag_dir: Path | None,
    diag_all: bool
) -> Eclipse:
    scst_metadata = metadata_from_scst(de)
    planned_tgts = planned_targets(scst_metadata)
    fixes = ok_aspect_fixes(aspect)
    legs = legs_by_circularity(fixes)

    new_recs = finish_eclipse(
        scst_metadata,
        planned_tgts,
        fixes,
        legs,
        aspect,
    )

    if (new_recs.deviant or diag_all) and diag_dir is not None:
        plot_diagnostic(
            diag_dir,
            new_recs,
            planned_tgts,
            fixes,
            legs,
        )

    return new_recs


def crunch_eclipse(
    de: DownloadedEclipse,
    aspect: DataFrame,
    diag_dir: Path | None,
    diag_all: bool,
) -> Eclipse:
    try:
        return crunch_eclipse_1(de, aspect, diag_dir, diag_all)
    except Exception as e:
        t = type(e).__name__
        m = str(e)
        if t == m:
            msg = m
        elif m == "":
            msg = t
        else:
            msg = f"{t}: {m}"
        raise EclipseCrunchError(f"eclipse {de.eclipse}: error: {msg}") from e


def load_scst_index(scst_dir: Path) -> dict[int, DownloadedEclipse]:
    with shelve.open(scst_dir / "index.shelf", "r") as shelf:
        return {
            int(k): DownloadedEclipse(
                eclipse = int(k),
                scst_file = scst_dir / v["scst"],
                nuv_has_raw6 = v["nuv_raw6"],
                fuv_has_raw6 = v["fuv_raw6"],
            )
            for k,v in shelf.items()
        }


def load_aspect_table(aspect_dir: Path, eclipse: list[int]) -> DataFrame:
    if eclipse:
        aspect = parquet.read_table(aspect_dir / "aspect.parquet",
                                    filters = [("eclipse", "in", set(eclipse))])
    else:
        aspect = parquet.read_table(aspect_dir / "aspect.parquet")
    return aspect.to_pandas().groupby("eclipse")

#
# Output
#

def arrow_type_for_py_type(t: type) -> pa.DataType:
    if t is int:
        return pa.uint64()
    if t is float:
        return pa.float64()
    if t is bool:
        return pa.bool_()
    if t is str:
        return pa.string()
    if t is bytes:
        return pa.binary()
    raise ValueError(f"don't know Arrow equiv for {t.__name__}")


class ColumnSponge:
    """Absorb a sequence of rows of a particular record type and
       convert them to a column-oriented pyarrow Table."""
    def __init__(self, dc: type):
        columns = {}
        fields = []
        for f in dc_fields(dc):
            columns[f.name] = []
            fields.append((f.name, arrow_type_for_py_type(f.type)))
        self.columns = columns
        self.schema = pa.schema(fields)

    def append(self, record: Any):
        for nm in self.schema.names:
            self.columns[nm].append(getattr(record, nm))

    def extend(self, records: Iterable[Any]):
        for rec in records:
            self.append(rec)

    def flush(self) -> pa.Table:
        columns = []
        for nm in self.schema.names:
            arr = pa.array(self.columns[nm], self.schema.field(nm).type)
            columns.append(arr)

        return pa.Table.from_arrays(columns, schema=self.schema)


class NewTableWriter:
    def __init__(self, output_dir: Path):
        self.metadata  = output_dir / "metadata.parquet"
        self.boresight = output_dir / "boresight.parquet"
        self.leg_aper  = output_dir / "leg-aperture.parquet"

        self.md_records  = ColumnSponge(MetadataRecord)
        self.bst_records = ColumnSponge(BoresightRecord)
        self.la_records  = ColumnSponge(LegApertureRecord)

    def __enter__(self) -> Self:
        return self

    def __exit__(self, ty: None | type, _val: Any, _tb: Any) -> None:
        # only write out the tables on normal exit
        if ty is None:
            self.flush()

    def add(self, rec: Eclipse) -> None:
        self.md_records.append(rec.metadata)
        self.bst_records.extend(rec.boresight)
        self.la_records.extend(rec.apertures)

    def flush(self) -> None:
        progress("compiling tables...")
        md  = self.md_records.flush()
        bst = self.bst_records.flush()
        la  = self.la_records.flush()

        md = md.sort_by([("eclipse", "ascending")])
        bst = bst.sort_by([("eclipse", "ascending"), ("leg", "ascending")])
        la = la.sort_by([("eclipse", "ascending"), ("leg", "ascending")])

        progress("writing tables...")
        parquet.write_table(md, self.metadata)
        parquet.write_table(bst, self.boresight)
        # this one has really big blobs and is known not to benefit from
        # dictionary encoding
        parquet.write_table(la, self.leg_aper,
                            use_dictionary=False, compression="zstd")
        progress("done.")


def process_update(total, completed, nom, dev, errors):
    progress(f"{nom} nominal + {dev} deviant + {errors} errors ="
             f" {completed} of {total} eclipses")


# This is a workaround for ProcessPoolExecutor trying to be too clever.
# See below.
def noop() -> None:
    pass


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("aspect_dir", type=Path,
                    help="find aspect tables in this directory")
    ap.add_argument("scst_dir", type=Path,
                    help="find indexed SCST files in this directory")
    ap.add_argument("output_dir", type=Path,
                    help="new tables will be written to this directory")
    ap.add_argument("eclipse", nargs="*", type=int,
                    help="process only these eclipses")
    ap.add_argument("--diag-images", type=Path, default=None,
                    help="write diagnostic images for hard eclipses"
                    " to this directory")
    ap.add_argument("--diag-all", action="store_true",
                    help="write diagnostic images for all eclipses")
    ap.add_argument("--debug", action="store_true",
                    help="print Python tracebacks for debugging")
    ap.add_argument("--stop-on-error", action="store_true",
                    help="stop on first error")
    args = ap.parse_args()

    # both of these directories must already exist
    args.aspect_dir = args.aspect_dir.resolve(strict=True)
    args.scst_dir = args.scst_dir.resolve(strict=True)

    # if these directories don't already exist, create them
    args.output_dir = args.output_dir.resolve(strict=False)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    if args.diag_images is not None:
        args.diag_images = args.diag_images.resolve(strict=False)
        args.diag_images.mkdir(parents=True, exist_ok=True)
        for old_image in list(args.diag_images.glob("*.png")):
            old_image.unlink()

    if args.diag_all and args.diag_images is None:
        ap.error("must have --diag-images with --diag-all")

    # We need to fork the worker processes _before_ loading tables,
    # so that only the parent process allocates memory for the tables,
    # otherwise we'll run the computer out of RAM and crash.
    # In order to force ProcessPoolExecutor to fork all the workers at
    # the right time, we have to specifically request the "fork" mode
    # of the underlying multiprocessing library, and we also have to
    # submit a no-op job to the pool immediately after creating it.
    ctx = multiprocessing.get_context("fork")
    with (
        ProcessPoolExecutor(mp_context=ctx) as pool,
        NewTableWriter(args.output_dir) as wr,
        open(args.output_dir / "deviant-eclipses.txt", "wt") as dev_log
    ):
        pool.submit(noop).result()

        progress("loading tables")
        scst_index = load_scst_index(args.scst_dir)
        aspect = load_aspect_table(args.aspect_dir, args.eclipse)

        progress("distributing eclipses...")
        futures = []
        for eclipse, asp in aspect:
            if len(futures) > 0 and len(futures) % 1000 == 0:
                progress(f"{len(futures)} eclipses distributed")
            futures.append(pool.submit(
                crunch_eclipse,
                scst_index[eclipse],
                asp,
                args.diag_images,
                args.diag_all,
            ))
        total = len(futures)
        if total > 0 and total % 1000 != 0:
            progress(f"{total} eclipses distributed")

        progress("processing eclipses...")

        errors = 0
        completed = 0
        nom = 0
        dev = 0
        for fut in as_completed(futures):
            try:
                result = fut.result()
                completed += 1
                wr.add(result)
                if result.deviant:
                    dev += 1
                    dev_log.write(f"{result.metadata.eclipse}\n")
                else:
                    nom += 1
                if completed % 1000 == 0:
                    process_update(total, completed, nom, dev, errors)

            except EclipseCrunchError as e:
                sys.stderr.write(e.args[0] + "\n")
                errors += 1

                if args.debug:
                    e.debug_report()
                if args.stop_on_error:
                    for f in futures:
                        f.cancel()
                    sys.exit(1)
                process_update(total, completed, nom, dev, errors)

        process_update(total, completed, nom, dev, errors)


main()
