import argparse
import csv
import itertools
import multiprocessing
import os
import sys
import time

from collections import namedtuple
from datetime import timedelta
from pathlib import Path

import numpy as np
import shapely
import pyarrow as pa

from gPhoton.coords.gnomonic import gnomfwd_simple, gnomrev_simple
from gPhoton.constants import DETSIZE, PIXEL_SIZE
from pyarrow import parquet

# hardcoded cdelt and cenpix parameters from load_aspect_solution
CDELT  = 1.0 / 36000.0
CENPIX = 0.0

# actual max time value in the tables:
# aspect:    1012654669.995
# boresight: 1012654536.995
MAXTIME    = 1100000000

# detector aperture radius in xi/eta units
APERTURE_RADIUS = (DETSIZE / CDELT) / 2

# Shapely has no concept of arcs; curves must be approximated using a
# sequence of lines.  The polygon generated by Point(x,y).buffer(r) is
# guaranteed to be inscribed within a true circle of radius r.  The
# approximation can be made more precise by increasing this parameter,
# which is the number of segments to use for each quarter-circle.
#
# At QUAD_SEGS = 72, the approximation of a circle with radius
# APERTURE_RADIUS has area 1262 cdelt^2 units smaller than a true
# circle with that radius; this is a relative error of 0.008%,
# which we should be able to live with.
QUAD_SEGS = 72

# data returned by gen_legs
LegInfo  = namedtuple("LegInfo",
                      ("legno", "t_min", "t_max", "ra0", "dec0", "roll0"))


def gen_legs(bores):
    # leg N starts at the time listed in row N of the boresight
    # table and continues until the time listed in row N+1, or to
    # end of eclipse if there is no row N+1.
    t_min = None
    ra0 = None
    dec0 = None
    roll0 = None
    legno = 1
    for row in bores.itertuples():
        if t_min is not None:
            yield LegInfo(legno, t_min, row.time, ra0, dec0, roll0)
            legno += 1
        t_min = row.time
        ra0 = row.ra0
        dec0 = row.dec0
        roll0 = row.roll0
    if t_min is not None:
        yield LegInfo(legno, t_min, MAXTIME, ra0, dec0, roll0)


def transform_to_xi_eta(leg, aspect):
    """convert an aspect solution to detector coordinates"""
    ra = aspect["ra"].to_numpy()
    dec = aspect["dec"].to_numpy()
    roll = aspect["roll"].to_numpy()
    return gnomfwd_simple(
        ra,
        dec,
        leg.ra0,
        leg.dec0,
        -roll,
        CDELT,
        CENPIX
    )


def ok_points(aspect):
    # if the mysterious 'flags' number is odd, it means this aspect
    # record should not be considered part of the observation.
    # the full set of reasons for flagging an aspect record is not
    # known, but the most common reason is that the telescope wasn't
    # oriented stably at that moment, e.g. because it was still
    # decelerating at the beginning of a leg.
    flagged = aspect["flags"].to_numpy() % 2 != 0

    # we also distrust aspect records immediately before and after
    # a flagged aspect record
    prev_flagged = np.concat([[False], flagged[:-1]]) & ~flagged
    next_flagged = np.concat([flagged[1:], [False]]) & ~flagged

    # we also distrust aspect records whose time step is not 1
    times = aspect["time"].to_numpy()
    timedelta_is_1 = np.insert(
        np.isclose(times[1:] - times[:-1], 1.0),
        0, True
    )

    return ~flagged & ~prev_flagged & ~next_flagged & timedelta_is_1


ProjectedExposureArea = namedtuple("ProjectedExposureArea", (
    "eclipse", "leg", "t_min", "t_max", "ra0", "dec0", "roll0",
    "full_exposure_time", "full_exposure_area", "partial_exposure_area"
))

def crunch_leg(eclipse, leg, aspect):
    xi, eta = transform_to_xi_eta(leg, aspect)
    ok = ok_points(aspect)

    # Because the criteria for a point being 'ok' include 'the
    # point's time step is 1 second', we can compute the total
    # exposure time by counting the number of True values in 'ok'.
    exposure_time = ok.sum()

    # The area that was exposed for the full exposure time of the leg
    # is the intersection of a set of APERTURE_RADIUS-sized disks, one
    # for each 'ok' point.  The area that was exposed for some but not
    # all of the leg is the union of the same disks, minus the
    # intersection.
    disks = shapely.buffer(
        shapely.points(xi[ok], eta[ok]),
        APERTURE_RADIUS,
        quad_segs=QUAD_SEGS,
    )
    full = shapely.intersection_all(disks)
    part = shapely.difference(shapely.union_all(disks), full)

    return ProjectedExposureArea(
        eclipse, leg.legno,
        leg.t_min, float(leg.t_max), leg.ra0, leg.dec0, leg.roll0,
        float(exposure_time), full, part
    )


def crunch_eclipse_1(eclipse, bst, asp):
    if len(asp) == 0:
        return

    for leg in gen_legs(bst):
        leg_min = leg.t_min
        leg_max = leg.t_max
        leg_asp = asp.query("@leg_min <= time < @leg_max")
        if len(leg_asp) > 0:
            try:
                yield crunch_leg(eclipse, leg, leg_asp)
            except Exception as e:
                raise RuntimeError(
                    f"error processing eclipse {eclipse}.{leg.legno}"
                ) from e


def crunch_eclipse(eclipse, bst, asp):
    return list(crunch_eclipse_1(eclipse, bst, asp))


def load_tables(d):
    def load(fname):
        return parquet.read_table(fname).to_pandas()

    progress("loading tables")

    aspect = load(d / "aspect.parquet").groupby("eclipse")
    boresight = load(d / "boresight.parquet").groupby("eclipse")
    metadata = load(d / "metadata.parquet").set_index("eclipse")

    return aspect, boresight, metadata


def project_exposures(aspect, boresight, metadata):
    # this is probably not the most efficient way to do this, but I
    # don't have the patience today to work out the most efficient way

    exposures = []
    with multiprocessing.Pool() as pool:
        progress("distributing eclipses")
        futures = []
        for eclipse, asp in aspect:
            if len(futures) > 0 and len(futures) % 1000 == 0:
                progress(f"{len(futures)} eclipses distributed")
                # early bail out on bugs in the worker
                for fut in futures:
                    if fut.ready() and not fut.successful():
                        fut.get() # will raise an exception

            try:
                obstype = metadata.loc[eclipse]["obstype"]
            except KeyError:
                continue
            if obstype == "unknown":
                continue

            try:
                bst = boresight.get_group(eclipse)
            except KeyError:
                continue
            if len(bst) == 0:
                continue

            futures.append(pool.apply_async(
                crunch_eclipse,
                (eclipse, bst, asp),
            ))

        progress(f"{len(futures)} eclipses distributed.")

        for n, fut in enumerate(futures):
            exposures.extend(fut.get())
            if (n+1) % 1000 == 0 and n+1 < len(futures):
                progress(f"{n+1}/{len(futures)} eclipses projected")

        progress(f"{n+1}/{len(futures)} eclipses projected.")

    return exposures


START_TIME = None
def progress(msg):
    now = time.monotonic()

    global START_TIME
    if START_TIME is None:
        START_TIME = now

    elapsed = timedelta(seconds=now - START_TIME)
    sys.stderr.write(f"[{elapsed}]  {msg}\n")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("dir", help="directory containing aspect data")
    args = ap.parse_args()
    d = Path(args.dir)

    aspect, boresight, metadata = load_tables(d)
    exposures = project_exposures(aspect, boresight, metadata)

    columns = []
    for f in ProjectedExposureArea._fields:
        progress(f"transposing exposure records: {f}")
        if f.endswith("_area"):
            columns.append(pa.array(getattr(r, f).wkb for r in exposures))
        else:
            columns.append(pa.array(getattr(r, f) for r in exposures))

    batch = pa.RecordBatch.from_arrays(columns, ProjectedExposureArea._fields)
    print(batch.schema)

    progress("writing parquet")
    parquet.write_table(pa.Table.from_batches([batch]), "exposure_rgns.parquet")

    progress("done")


main()
