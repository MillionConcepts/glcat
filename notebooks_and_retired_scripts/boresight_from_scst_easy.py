import argparse
import csv
import itertools
import multiprocessing
import os
import shelve
import shutil
import sys
import time

from collections import namedtuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, fields as dc_fields
from datetime import datetime, timedelta, timezone
from math import isnan, inf as Inf, nan as NaN, pi as PI
from pathlib import Path
from typing import Any, Iterable, Self

import numpy as np
import pyarrow as pa
import shapely

from astropy.coordinates import angular_separation
from astropy.io import fits
from gPhoton.constants import DETSIZE, PIXEL_SIZE
from gPhoton.coords.gnomonic import gnomfwd_simple, gnomrev_simple
from gPhoton.io import mast
from pandas import DataFrame
from pyarrow import parquet
from scipy.spatial.distance import pdist

# conversion factors for angular units
DEG2RAD = PI/180
RAD2DEG = 180/PI

# Separation limit: if any points are farther than this from a
# planned target then we cannot easily assign them to that target.
# This threshold is a nice round number which leads to an
# uncircularity slightly less than SR_ISOP_PROBLEM_THRESHOLD
# (see below).  8% of the diameter of the detector = 6 minutes of arc.
TARGET_MAX_SEP = DETSIZE * 0.08

# hardcoded cdelt and cenpix parameters from load_aspect_solution
CDELT  = 1.0 / 36000.0
CENPIX = 0.0

# conversion from distance in cdelt units to distance in arcsec
CDELT_TO_ARCSEC = 0.1

# conversion from area in square cdelt pixels to area in square arcsec
CDELT2_TO_ARCSEC2 = 0.01

# conversion from area in square cdelt pixels to area in square degrees
CDELT2_TO_DEG2 = CDELT2_TO_ARCSEC2 / (3600 * 3600)

# detector aperture radius in xi/eta units
APERTURE_RADIUS = (DETSIZE / CDELT) / 2

# Shapely has no concept of arcs; curves must be approximated using a
# sequence of lines.  The polygon generated by Point(x,y).buffer(r) is
# guaranteed to be inscribed within a true circle of radius r.  The
# approximation can be made more precise by increasing this parameter,
# which is the number of segments to use for each quarter-circle.
#
# At QUAD_SEGS = 72, the approximation of a circle with radius
# APERTURE_RADIUS has area 1262 cdelt^2 units smaller than a true
# circle with that radius; this is a relative error of 0.008%,
# which we should be able to live with.
QUAD_SEGS = 72

# The difference between the GPS epoch (1980-01-06T00:00:00Z) and the
# Unix epoch (1970-01-01T00:00:00Z).  SCST and RAW6 files both contain
# numeric timestamps counting from the GPS epoch.
GPS_EPOCH = datetime(1980, 1, 6, 0, 0, 0, 0, timezone.utc).timestamp()

# If the shifted reciprocal isoperimetric quotient of the fully
# exposed region is larger than this, the eclipse needs to be analyzed
# in detail for problems.
SR_ISOP_PROBLEM_THRESHOLD = 10 ** -2.5

# If the ratio of the area of the partially exposed region to the area
# of the fully exposed region is greater than this, the eclipse needs
# to be analyzed in detail for problems.  The partially exposed region
# can get quite large without its actually being a problem, notably
# for petal-pattern eclipses, and the isoperimetric quotient catches
# most of the problem cases we care about; this mainly exists to deal
# with the petal-pattern eclipses where the pattern kept its radial
# symmetry but became abnormally large.
AREA_RATIO_PROBLEM_THRESHOLD = 0.75

# Legs this short are not useful.  Also, legs this short often consist
# of just one or two points that _should have_ been flagged but weren't.
# TODO Tune cutoff.
MIN_LEG_EXPOSURE_TIME = 30


START_TIME = None
def progress(msg):
    now = time.monotonic()

    global START_TIME
    if START_TIME is None:
        START_TIME = now

    elapsed = timedelta(seconds=now - START_TIME)
    sys.stderr.write(f"[{elapsed}]  {msg}\n")


def angular_separation_deg(ra1, dec1, ra2, dec2):
    return RAD2DEG * angular_separation(
        ra1 * DEG2RAD,
        dec1 * DEG2RAD,
        ra2 * DEG2RAD,
        dec2 * DEG2RAD
    )


@dataclass(frozen=True, slots=True)
class DownloadedEclipse:
    eclipse: int
    scst_file: Path
    nuv_has_raw6: bool
    fuv_has_raw6: bool


@dataclass(frozen=True, slots=True)
class SCSTMetadata:
    eclipse: int
    visit: int
    plan_type: str
    plan_subtype: str
    plan_id: int
    planned_legs: int

    eclipse_start: float
    eclipse_length: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_consistent: bool
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_consistent: bool
    fuv_has_raw6: bool

    roll: float
    legs: list[tuple[float, float]]
    leg1_alt: None | tuple[float, float]


@dataclass(slots=True, frozen=True)
class EasyMetadataRecord:
    eclipse: int
    plan_type: str
    plan_subtype: str
    visit: int
    plan_id: int
    planned_legs: int
    observed_legs: int

    eclipse_start: float
    eclipse_length: float
    ok_exposure_time: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_consistent: bool
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_consistent: bool
    fuv_has_raw6: bool

    ra_min: float
    ra_max: float
    dec_min: float
    dec_max: float


@dataclass(slots=True, frozen=True)
class HardMetadataRecord:
    eclipse: int
    reason: str
    plan_type: str
    plan_subtype: str
    visit: int
    plan_id: int
    planned_legs: int

    eclipse_start: float
    eclipse_length: float
    ok_exposure_time: float

    nuv_det_on_time: float
    nuv_det_temp: float
    nuv_tdc_temp: float
    nuv_consistent: bool
    nuv_has_raw6: bool

    fuv_det_on_time: float
    fuv_det_temp: float
    fuv_tdc_temp: float
    fuv_consistent: bool
    fuv_has_raw6: bool


@dataclass(slots=True, frozen=True)
class NewBoresightRecord:
    eclipse: int
    leg: int

    time: float
    length: float

    ra0: float
    dec0: float
    roll0: float

    ra_min: float
    ra_max: float
    dec_min: float
    dec_max: float

    planned_ra: float
    planned_dec: float

    full_exposure_area: float
    full_exposure_uncircularity: float
    partial_exposure_area_ratio: float


@dataclass(slots=True, frozen=True)
class LegApertureRecord:
    eclipse: int
    leg: int

    full_exposure_region: bytes
    partial_exposure_region: bytes



def scst_timestamp(stamp: str) -> float:
    """Parse a string timestamp from the format found in SCST file
       headers.  Returns a floating-point count of seconds since the GPS
       epoch.  (This epoch makes them directly comparable to the
       *numeric* timestamps found in the SCST file's *data.*)"""
    return datetime.strptime(stamp, "%y%m%dT%H%M%S%z").timestamp() - GPS_EPOCH


def temp_records_consistent(
    on_time: float,
    det_temp: float,
    tdc_temp: float,
) -> bool:
    """True if the detector temperature fields for a particular band are
       self-consistent.

       If ON_TIME is zero, both DET_TEMP and TDC_TEMP should be NaN.
       If ON_TIME is positive, both DET_TEMP and TDC_TEMP should be positive.
       None of the fields should be negative.
    """
    if on_time == 0:
        return isnan(det_temp) and isnan(tdc_temp)
    if on_time > 0:
        return det_temp > 0 and tdc_temp > 0
    return False


def metadata_from_scst(de: DownloadedEclipse) -> SCSTMetadata:
    """Extract all of the per-eclipse metadata that we care about from
       an SCST file."""
    with fits.open(de.scst_file) as hdulist:
        header = hdulist[0].header

        # For whatever reason, the numeric timestamps in an SCST
        # file's actual data are almost always nnnnnn.995 -- that is,
        # 5000 μs *before* a whole number of seconds since the epoch.
        # The eclipse start time, however, is expressed as a string
        # timestamp with no subsecond information.  Fudge the start
        # time 0.01 s (10,000 μs) earlier to ensure that it will
        # compare strictly less than any data timestamp.
        eclipse_start = scst_timestamp(header["TRANGE0"]) - 0.01

        nuv_det_on_time = header.get('NHVNOMN', 0)
        fuv_det_on_time = header.get('NHVNOMF', 0)
        nuv_det_temp    = header.get('NDTDET', NaN)
        nuv_tdc_temp    = header.get('NDTTDC', NaN)
        fuv_det_temp    = header.get('FDTDET', NaN)
        fuv_tdc_temp    = header.get('FDTTDC', NaN)

        ra0  = header["RA_CENT"]
        dec0 = header["DEC_CENT"]
        ra1  = header.get("RA_1")
        dec1 = header.get("DEC_1")

        if (ra1 == ra0 or ra1 is None) and (dec1 == dec0 or dec1 is None):
            leg1_alt = None
        else:
            assert ra1 is not None, f"ra1={ra1} but dec1={dec1}"
            assert dec1 is not None, f"ra1={ra1} but dec1={dec1}"
            leg1_alt = (ra1, dec1)

        legs = [ (ra0, dec0) ]
        planned_legs = max(1, header["MPSNPOS"])
        if planned_legs >= 2:
            for i in range(2, planned_legs + 1):
                legs.append((header.get(f"RA_{i}"), header.get(f"DEC_{i}")))

        return SCSTMetadata(
            eclipse            = header["ECLIPSE"],
            visit              = header["VISIT"],
            plan_type          = header["MPSTYPE"],
            plan_subtype       = header["MPSPLAN"],
            plan_id            = header["PLANID"],
            planned_legs       = planned_legs,

            # As the stop time is also expressed as a string timestamp,
            # it will compare greater than any data timestamp from this
            # eclipse with no need for any fudging.
            eclipse_start      = eclipse_start,
            eclipse_length     = (scst_timestamp(header["TRANGE1"])
                                  - eclipse_start),

            nuv_det_on_time    = nuv_det_on_time,
            nuv_det_temp       = nuv_det_temp,
            nuv_tdc_temp       = nuv_tdc_temp,
            nuv_consistent     = temp_records_consistent(nuv_det_on_time,
                                                  nuv_det_temp,
                                                  nuv_tdc_temp),
            nuv_has_raw6       = de.nuv_has_raw6,

            fuv_det_on_time    = fuv_det_on_time,
            fuv_det_temp       = fuv_det_temp,
            fuv_tdc_temp       = fuv_tdc_temp,
            fuv_consistent     = temp_records_consistent(fuv_det_on_time,
                                                  fuv_det_temp,
                                                  fuv_tdc_temp),
            fuv_has_raw6       = de.fuv_has_raw6,

            roll               = header["ROLL"],
            legs               = legs,
            leg1_alt           = leg1_alt,
        )


def transform_to_xi_eta(
    ra0: float,
    dec0: float,
    ra: np.ndarray,
    dec: np.ndarray,
    roll: np.ndarray
) -> (np.ndarray, np.ndarray):
    """convert a sequence of equatorial sky coordinates to detector
       coordinates, taking (ra0, dec0) as the sky coordinate corresponding
       to detector (0,0)."""
    ra, dec, roll = np.broadcast_arrays(ra, dec, roll)
    return gnomfwd_simple(
        ra,
        dec,
        ra0,
        dec0,
        -roll,
        CDELT,
        CENPIX
    )


def transform_to_ra_dec(
    ra0: float,
    dec0: float,
    xi: np.ndarray,
    eta: np.ndarray,
) -> (np.ndarray, np.ndarray):
    """convert a sequence of detector coordinates to equatorial sky
       coordinates, taking (ra0, dec0) as the sky coordinate corresponding
       to detector (0,0)."""
    return gnomrev_simple(
        xi,
        eta,
        ra0,
        dec0,
        0,
        CDELT,
        CENPIX,
        CENPIX
    )


def ok_aspect_points(aspect: DataFrame) -> (
        np.ndarray, np.ndarray, np.ndarray, np.ndarray,
):
    """extract the time of each aspect fix from the aspect table,
       calculate whether each aspect fix is considered part of the
       observation, and return (time, ra, dec, roll) for each fix
       that is considered part of the observation."""
    # if the mysterious 'flags' number is odd, it means this aspect
    # record should not be considered part of the observation.
    # the full set of reasons for flagging an aspect record is not
    # known, but the most common reasons are that the detector hadn't
    # yet come up to operating voltage, or that the telescope was
    # carrying out a high-speed slew during that time step.
    flagged = aspect["flags"].to_numpy() % 2 != 0

    # we also distrust aspect records immediately before and after
    # a flagged aspect record
    prev_flagged = np.concat([[False], flagged[:-1]]) & ~flagged
    next_flagged = np.concat([flagged[1:], [False]]) & ~flagged

    # we also distrust aspect records whose time step is not 1
    times = aspect["time"].to_numpy()
    timedelta_is_1 = np.insert(
        np.isclose(times[1:] - times[:-1], 1.0),
        0, True
    )

    ok = ~flagged & ~prev_flagged & ~next_flagged & timedelta_is_1

    return (
        times[ok],
        aspect["ra"].to_numpy()[ok],
        aspect["dec"].to_numpy()[ok],
        aspect["roll"].to_numpy()[ok],
    )


def exposure_regions(ap_points):
    """
    Find the area, on an imaginary detector plate of unbounded
    extent, that was exposed to incoming light during one or more of
    the 'ok' time steps.  This is simply the geometric union of the
    detector apertures for each 'ok' time step, and each of those
    apertures is a disk centered on the ap_point for that time step.

    Also find the area that was exposed to light during _all_ of the
    'ok' time steps, and compute two measures of how stable the
    telescope seems to have been during the exposure.
    """
    # for each 'ok' aspect point, find the detector aperture with that
    # point at its center
    ap_disks = shapely.buffer(
        ap_points,
        APERTURE_RADIUS,
        quad_segs=QUAD_SEGS,
    )

    # The union of all the ap_disks gives the area (on an imaginary
    # photographic plate of unbounded extent) that was exposed to
    # incoming light; the intersection of all the ap_disks gives the
    # area that was exposed for the full exposure time of the leg;
    # and finally, the difference (union minus intersection) is the
    # region of the imaginary plate that didn't get fully exposed,
    # therefore source finding is likely to be unreliable within.
    full = shapely.intersection_all(ap_disks)
    edge = shapely.difference(shapely.union_all(ap_disks), full)

    # for AIS eclipses there might be no overlap at all when
    # considering the entire eclipse at once
    if full.is_empty:
        full_area = 0
        s_r_isop = Inf
        pa_ratio = Inf
    else:
        # Compute a mostly standard measure of compactness for the region
        # of full exposure: P²⁄4πA - 1, which we're calling the shifted
        # reciprocal isoperimetric quotient.  (The standard isoperimetric
        # quotient is 4πA⁄P², which ranges from 0 to 1, where 1 is a
        # perfect circle and 0 is a shape with zero area but nonzero
        # perimeter, e.g. a line segment.  The reciprocal ranges from 1 to
        # positive infinity, and subtracting 1 puts a perfect circle at 0.
        # We do this transformation because most legs have an isoperimetric
        # quotient very close to 1; on our data set, the shifted reciprocal
        # quotient ranges from 4 × 10⁻⁵ to 0.21, which is usefully
        # put on a log scale.)

        # I don't trust shapely not to recalculate these properties on
        # every access.
        full_perimeter = full.exterior.length
        full_area = full.area
        s_r_isop = full_perimeter * full_perimeter / (4 * np.pi * full_area) - 1

        # Also compute the ratio of the area of the partially exposed
        # region to the area of the fully exposed region.
        pa_ratio = edge.area / full_area

    return ap_disks, full, edge, full_area, s_r_isop, pa_ratio


def planned_targets(scst_metadata):
    planned_ra0 = [c[0] for c in scst_metadata.legs]
    planned_dec0 = [c[1] for c in scst_metadata.legs]
    if scst_metadata.leg1_alt is not None:
        planned_ra0.append(scst_metadata.leg1_alt[0])
        planned_dec0.append(scst_metadata.leg1_alt[1])
    return np.array(planned_ra0), np.array(planned_dec0)


def easy_assign_points_to_targets(
    planned_ra0: np.ndarray,
    planned_dec0: np.ndarray,
    ra: np.ndarray,
    dec: np.ndarray,
) -> np.ndarray:
    """
    Given a series of planned targets, zip(planned_ra0, planned_dec0),
    and a series of unflagged aspect points, zip(ra, dec), compute the
    angular separation from each aspect point to each target and
    assign points to a target if they are closer than TARGET_MAX_SEP
    to that target.  Returns a vector of labels:
    labels[i] == j means aspect point i is assigned to target j.
    labels[i] == -1 means aspect point i could not be assigned this way.
    """
    assert ra.shape == dec.shape, f"{ra.shape=} != {dec.shape=}"
    assert planned_ra0.shape == planned_dec0.shape, \
        f"{planned_ra0.shape=} != {planned_dec0.shape=}"

    # If there's more than one target and any pair of them is closer
    # to each other than TARGET_MAX_SEP, reduce the cutoff to 1/3 of
    # the distance between the closest two targets.
    if len(planned_ra0) == 1:
        target_max_sep = TARGET_MAX_SEP
    else:
        target_seps = pdist(
            np.hstack([
                planned_ra0[:, np.newaxis],
                planned_dec0[:, np.newaxis]
            ]),
            metric = lambda x, y: angular_separation_deg(x[0], x[1], y[0], y[1])
        )
        target_max_sep = min(TARGET_MAX_SEP, target_seps.min().item() / 3)

    labels = np.full(ra.shape, -1, np.int32)
    for i, (ra0, dec0) in enumerate(zip(planned_ra0, planned_dec0)):
        unassigned = labels == -1
        d = angular_separation_deg(ra0, dec0, ra[unassigned], dec[unassigned])
        unassigned[unassigned] = d < target_max_sep
        # unassigned is now True for each element that _should_ be assigned
        # to target i
        labels[unassigned] = i

    return labels


def eclipse_not_easy(
    reason: str,
    md: SCSTMetadata,
    ok_exposure_time: int
) -> (
    str, HardMetadataRecord,
    list[NewBoresightRecord], list[LegApertureRecord]
):
    return (
        HardMetadataRecord(
            eclipse          = md.eclipse,
            reason           = reason,
            visit            = md.visit,
            plan_type        = md.plan_type,
            plan_subtype     = md.plan_subtype,
            plan_id          = md.plan_id,
            planned_legs     = md.planned_legs,

            eclipse_start    = md.eclipse_start,
            eclipse_length   = md.eclipse_length,
            ok_exposure_time = ok_exposure_time,

            nuv_det_on_time  = md.nuv_det_on_time,
            nuv_det_temp     = md.nuv_det_temp,
            nuv_tdc_temp     = md.nuv_tdc_temp,
            nuv_consistent   = md.nuv_consistent,
            nuv_has_raw6     = md.nuv_has_raw6,

            fuv_det_on_time  = md.fuv_det_on_time,
            fuv_det_temp     = md.fuv_det_temp,
            fuv_tdc_temp     = md.fuv_tdc_temp,
            fuv_consistent   = md.fuv_consistent,
            fuv_has_raw6     = md.fuv_has_raw6,
        ),
        [],
        []
    )

def relabel_ascending(assignment: np.ndarray, observed: int) -> np.ndarray:
    relabeled = np.empty_like(assignment)
    mask = assignment == -1
    relabeled[mask] = -1

    new_label = 0
    while not mask.all():
        old_label = assignment[~mask][0]
        omask = assignment == old_label
        relabeled[omask] = new_label
        new_label += 1
        mask |= omask

    assert new_label == observed, f"{new_label} != {observed}"
    return relabeled


def boresight_records(
    eclipse: int,
    leg_index: int,
    ra0: float,
    dec0: float,
    time: np.ndarray,
    ra: np.ndarray,
    dec: np.ndarray,
    roll: np.ndarray,
    leg_assignment: np.ndarray
) -> (
    str,
    EasyMetadataRecord,
    list[NewBoresightRecord],
    list[LegApertureRecord]
):
    this_leg = leg_assignment == leg_index
    time = time[this_leg]
    ra = ra[this_leg]
    dec = dec[this_leg]
    roll = roll[this_leg]
    xi, eta = transform_to_xi_eta(ra0, dec0, ra, dec, roll)

    ap_points = shapely.points(xi, eta)
    ap_disks, full, edge, full_area, s_r_isop, pa_ratio = \
        exposure_regions(ap_points)

    if len(time) == 1:
        # Corner case if the leg is a single point, the partial area
        # will be vacuous.  This should be weeded out earlier via
        # the MIN_LEG_EXPOSURE_TIME filter, but let's be careful.
        bbox = full.bounds
    else:
        bbox = edge.bounds
        # because of how the regions are constructed, the partial
        # exposure region's bounding box should always contain the
        # full exposure region's bounding box
        assert shapely.box(*bbox).contains(shapely.box(*full.bounds)), \
            f"{bbox!r} does not contain {full.bounds!r}"

    # use the centroid of the full exposure region as the "observed center"
    # of the leg
    observed_center = full.centroid

    # convert bbox and centroid to sky coordinates
    ra_bounds, dec_bounds = transform_to_ra_dec(
        ra0, dec0,
        np.array([bbox[0], observed_center.x, bbox[2]]),
        np.array([bbox[1], observed_center.y, bbox[3]])
    )

    # aspect time points all end in .995 for some reason.
    # double check this, then take advantage of it to make the
    # boresight start and end times be strictly less and greater
    # than the range of good aspect time steps.
    assert all(np.isclose(time, np.rint(time) - 0.005)), \
        "times don't all end with .995"
    start_time = time[0] - 0.005
    stop_time = time[-1] + 0.005
    length = stop_time + 1 - start_time

    leg_id = leg_index + 1

    return (
        NewBoresightRecord(
            eclipse                     = eclipse,
            leg                         = leg_id,
            time                        = start_time,
            length                      = length,
            ra0                         = ra_bounds[1],
            dec0                        = dec_bounds[1],
            roll0                       = roll[0],
            planned_ra                  = ra0,
            planned_dec                 = dec0,
            ra_min                      = ra_bounds[0],
            ra_max                      = ra_bounds[2],
            dec_min                     = dec_bounds[0],
            dec_max                     = dec_bounds[2],
            full_exposure_area          = full_area,
            full_exposure_uncircularity = s_r_isop,
            partial_exposure_area_ratio = pa_ratio,
        ),
        LegApertureRecord(
            eclipse                 = eclipse,
            leg                     = leg_id,
            full_exposure_region    = full.wkb,
            partial_exposure_region = edge.wkb,
        )
    )


def eclipse_easy(
    md: SCSTMetadata,
    planned_ra0: np.ndarray,
    planned_dec0: np.ndarray,
    time: np.ndarray,
    ra: np.ndarray,
    dec: np.ndarray,
    roll: np.ndarray,
    leg_assignment: np.ndarray,
    observed_legs: int
) -> (
    EasyMetadataRecord,
    list[NewBoresightRecord],
    list[LegApertureRecord]
):
    boresight_recs = []
    la_recs = []
    leg_assignment = relabel_ascending(leg_assignment, observed_legs)

    for i in range(0, observed_legs):
        br, la = boresight_records(
            md.eclipse, i, planned_ra0[i], planned_dec0[i],
            time, ra, dec, roll, leg_assignment
        )
        boresight_recs.append(br)
        la_recs.append(la)

    ra_min = min(br.ra_min for br in boresight_recs)
    ra_max = max(br.ra_max for br in boresight_recs)
    dec_min = min(br.dec_min for br in boresight_recs)
    dec_max = max(br.dec_max for br in boresight_recs)

    return (
        EasyMetadataRecord(
            eclipse          = md.eclipse,
            visit            = md.visit,
            plan_type        = md.plan_type,
            plan_subtype     = md.plan_subtype,
            plan_id          = md.plan_id,
            planned_legs     = md.planned_legs,
            observed_legs    = observed_legs,

            eclipse_start    = md.eclipse_start,
            eclipse_length   = md.eclipse_length,
            ok_exposure_time = len(time),

            nuv_det_on_time  = md.nuv_det_on_time,
            nuv_det_temp     = md.nuv_det_temp,
            nuv_tdc_temp     = md.nuv_tdc_temp,
            nuv_consistent   = md.nuv_consistent,
            nuv_has_raw6     = md.nuv_has_raw6,

            fuv_det_on_time  = md.fuv_det_on_time,
            fuv_det_temp     = md.fuv_det_temp,
            fuv_tdc_temp     = md.fuv_tdc_temp,
            fuv_consistent   = md.fuv_consistent,
            fuv_has_raw6     = md.fuv_has_raw6,

            ra_min           = ra_min,
            ra_max           = ra_max,
            dec_min          = dec_min,
            dec_max          = dec_max,
        ),
        boresight_recs,
        la_recs
    )


def crunch_eclipse_1(
    de: DownloadedEclipse,
    aspect: DataFrame
) -> (
    EasyMetadataRecord | HardMetadataRecord,
    list[NewBoresightRecord],
    list[LegApertureRecord]
):
    scst_metadata = metadata_from_scst(de)
    time, ra, dec, roll = ok_aspect_points(aspect)

    planned_ra0, planned_dec0 = planned_targets(scst_metadata)
    leg_assignment = easy_assign_points_to_targets(
        planned_ra0, planned_dec0,
        ra, dec,
    )

    if (leg_assignment < 0).any():
        return eclipse_not_easy(
            "points out of easy range",
            scst_metadata,
            len(time)
        )

    observed_legs = 0
    for i in range(leg_assignment.max() + 1):
        leg_i = leg_assignment == i
        n_leg_i = np.count_nonzero(leg_i)
        if n_leg_i == 0:
            # this leg was not observed
            continue
        if n_leg_i < MIN_LEG_EXPOSURE_TIME:
            # this leg is too short
            leg_assignment[leg_i] = -1
            continue
        observed_legs += 1
        first_leg_i, last_leg_i = np.where(leg_i)[0][[0, -1]]
        if n_leg_i < last_leg_i + 1 - first_leg_i:
            return eclipse_not_easy(
                "legs not continuous",
                scst_metadata,
                len(time)
            )

    if not observed_legs:
        return eclipse_not_easy("no legs", scst_metadata, len(time))

    return eclipse_easy(
        scst_metadata,
        planned_ra0, planned_dec0,
        time, ra, dec, roll,
        leg_assignment, observed_legs
    )


class EclipseCrunchError(RuntimeError):
    pass


def crunch_eclipse(
    de: DownloadedEclipse,
    aspect: DataFrame
) -> (
    EasyMetadataRecord | HardMetadataRecord,
    list[NewBoresightRecord],
    list[LegApertureRecord]
):
    try:
        return crunch_eclipse_1(de, aspect)
    except Exception as e:
        t = type(e).__name__
        m = str(e)
        if t == m:
            msg = m
        elif m == "":
            msg = t
        else:
            msg = f"{t}: {m}"
        raise EclipseCrunchError(f"eclipse {de.eclipse}: error: {msg}") from e


def load_scst_index(scst_dir: Path) -> dict[int, DownloadedEclipse]:
    with shelve.open(scst_dir / "index.shelf", "r") as shelf:
        return {
            int(k): DownloadedEclipse(
                eclipse = int(k),
                scst_file = scst_dir / v["scst"],
                nuv_has_raw6 = v["nuv_raw6"],
                fuv_has_raw6 = v["fuv_raw6"],
            )
            for k,v in shelf.items()
        }


def load_aspect_table(aspect_dir: Path, eclipse: list[int]) -> DataFrame:
    if eclipse:
        aspect = parquet.read_table(aspect_dir / "aspect.parquet",
                                    filters = [("eclipse", "in", set(eclipse))])
    else:
        aspect = parquet.read_table(aspect_dir / "aspect.parquet")
    return aspect.to_pandas().groupby("eclipse")

#
# Output
#

def arrow_type_for_py_type(t: type) -> pa.DataType:
    if t is int:
        return pa.uint64()
    if t is float:
        return pa.float64()
    if t is bool:
        return pa.bool_()
    if t is str:
        return pa.string()
    if t is bytes:
        return pa.binary()
    raise ValueError(f"don't know Arrow equiv for {t.__name__}")


class ColumnSponge:
    """Absorb a sequence of rows of a particular record type and
       convert them to a column-oriented pyarrow Table."""
    def __init__(self, dc: type):
        columns = {}
        fields = []
        for f in dc_fields(dc):
            columns[f.name] = []
            fields.append((f.name, arrow_type_for_py_type(f.type)))
        self.columns = columns
        self.schema = pa.schema(fields)

    def append(self, record: Any):
        for nm in self.schema.names:
            self.columns[nm].append(getattr(record, nm))

    def extend(self, records: Iterable[Any]):
        for rec in records:
            self.append(rec)

    def flush(self) -> pa.Table:
        columns = []
        for nm in self.schema.names:
            arr = pa.array(self.columns[nm], self.schema.field(nm).type)
            columns.append(arr)

        return pa.Table.from_arrays(columns, schema=self.schema)


class NewTableWriter:
    def __init__(self, output_dir: Path):
        self.easy_metadata  = output_dir / "metadata-easy.parquet"
        self.easy_boresight = output_dir / "boresight-easy.parquet"
        self.easy_leg_aper  = output_dir / "leg-aperture-easy.parquet"
        self.hard_metadata  = output_dir / "hard-eclipses.parquet"

        self.easy_md_records  = ColumnSponge(EasyMetadataRecord)
        self.easy_bst_records = ColumnSponge(NewBoresightRecord)
        self.easy_la_records  = ColumnSponge(LegApertureRecord)
        self.hard_md_records  = ColumnSponge(HardMetadataRecord)

    def __enter__(self) -> Self:
        return self

    def __exit__(self, ty: None | type, _val: Any, _tb: Any) -> None:
        # only write out the tables on normal exit
        if ty is None:
            self.flush()

    def add_easy(self, md, bst, la) -> None:
        self.easy_md_records.append(md)
        self.easy_bst_records.extend(bst)
        self.easy_la_records.extend(la)

    def add_hard(self, md) -> None:
        self.hard_md_records.append(md)

    def flush(self) -> None:
        progress("compiling tables...")
        easy_md  = self.easy_md_records.flush()
        hard_md  = self.hard_md_records.flush()
        easy_bst = self.easy_bst_records.flush()
        easy_la  = self.easy_la_records.flush()

        easy_md = easy_md.sort_by([("eclipse", "ascending")])
        hard_md = hard_md.sort_by([("reason", "ascending"),
                                   ("eclipse", "ascending")])
        easy_bst = easy_bst.sort_by([("eclipse", "ascending"),
                                     ("leg", "ascending")])
        easy_la = easy_la.sort_by([("eclipse", "ascending"),
                                   ("leg", "ascending")])

        progress("writing tables...")
        parquet.write_table(easy_md,  self.easy_metadata)
        parquet.write_table(hard_md,  self.hard_metadata)
        parquet.write_table(easy_bst, self.easy_boresight)
        # this one has really big blobs and is known not to benefit from
        # dictionary encoding
        parquet.write_table(easy_la,  self.easy_leg_aper,
                            use_dictionary=False, compression="zstd")
        progress("done.")


def process_update(total, completed, easy, oor, disc, noleg, errors):
    progress(f"{easy} easy + {oor} oor + {disc} disc + "
             f"{noleg} no legs + {errors} errors ="
             f" {completed} of {total} eclipses")


# This is a workaround for ProcessPoolExecutor trying to be too clever.
# See below.
def noop() -> None:
    pass


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("aspect_dir", type=Path,
                    help="find aspect tables in this directory")
    ap.add_argument("scst_dir", type=Path,
                    help="find indexed SCST files in this directory")
    ap.add_argument("output_dir", type=Path,
                    help="new tables will be written to this directory")
    ap.add_argument("eclipse", nargs="*", type=int,
                    help="process only these eclipses")
    ap.add_argument("--debug", action="store_true",
                    help="print Python tracebacks for debugging")
    ap.add_argument("--stop-on-error", action="store_true",
                    help="stop on first error")
    args = ap.parse_args()

    # both of these directories must already exist
    args.aspect_dir = args.aspect_dir.resolve(strict=True)
    args.scst_dir = args.scst_dir.resolve(strict=True)

    # if this directory doesn't already exist, create it
    args.output_dir = args.output_dir.resolve(strict=False)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # We need to fork the worker processes _before_ loading tables,
    # so that only the parent process allocates memory for the tables,
    # otherwise we'll run the computer out of RAM and crash.
    # In order to force ProcessPoolExecutor to fork all the workers at
    # the right time, we have to specifically request the "fork" mode
    # of the underlying multiprocessing library, and we also have to
    # submit a no-op job to the pool immediately after creating it.
    ctx = multiprocessing.get_context("fork")
    with (
        ProcessPoolExecutor(mp_context=ctx) as pool,
        NewTableWriter(args.output_dir) as wr,
    ):
        pool.submit(noop).result()

        progress("loading tables")
        scst_index = load_scst_index(args.scst_dir)
        aspect = load_aspect_table(args.aspect_dir, args.eclipse)

        progress("distributing eclipses...")
        futures = []
        for eclipse, asp in aspect:
            if len(futures) > 0 and len(futures) % 1000 == 0:
                progress(f"{len(futures)} eclipses distributed")
            futures.append(pool.submit(
                crunch_eclipse,
                scst_index[eclipse],
                asp
            ))
        total = len(futures)
        if total > 0 and total % 1000 != 0:
            progress(f"{total} eclipses distributed")

        progress("processing eclipses...")

        errors = 0
        completed = 0
        easy = 0
        oor = 0
        noleg = 0
        disc = 0
        for fut in as_completed(futures):
            try:
                md, nbs, nla = fut.result()
                completed += 1
                match md:
                    case EasyMetadataRecord():
                        easy += 1
                        wr.add_easy(md, nbs, nla)
                        if completed % 1000 == 0:
                            process_update(total, completed,
                                           easy, oor, disc, noleg, errors)
                    case HardMetadataRecord:
                        assert not nbs
                        assert not nla
                        if md.reason == "legs not continuous":
                            disc += 1
                        elif md.reason == "no legs":
                            noleg += 1
                        else:
                            oor += 1
                        wr.add_hard(md)
                        process_update(total, completed,
                                       easy, oor, disc, noleg, errors)

            except EclipseCrunchError as e:
                sys.stderr.write(e.args[0] + "\n")
                errors += 1

                if args.debug:
                    if hasattr(e.__cause__, "tb"):
                        skipping = True
                        for line in e.__cause__.tb.splitlines():
                            if skipping:
                                if line == '"""':
                                    skipping = False
                            else:
                                if line == "The above exception was the direct cause of the following exception:":
                                    break
                                sys.stderr.write(line + "\n")
                    else:
                        import traceback
                        traceback.print_exc(e.__cause__)

                if args.stop_on_error:
                    sys.exit(1)
                process_update(total, completed,
                               easy, oor, disc, noleg, errors)

        process_update(total, completed, easy, oor, disc, noleg, errors)


main()
